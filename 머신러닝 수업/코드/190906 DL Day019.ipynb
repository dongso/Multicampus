{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"190906 DL Day019.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"e2JngWG2542z","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","import numpy as np"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_C9O5kA16IES","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rTzOJMgJ6KPf","colab_type":"text"},"source":["# Sequence-to-Sequence (seq2seq)\n","\n","seq2seq: 주로 기계번역에 많이 사용된다.\n","* 입력을 위한 신경망인 인코더, 인코더와 디코더 사이의 컨텍스트 벡서, 출력을 위한 신경망인 디코더로 구성되어있다.\n","* 트레이닝 과정과 테스트 과정이 다르다"]},{"cell_type":"markdown","metadata":{"id":"_YR0aRL6AvjS","colab_type":"text"},"source":["# Seq2Seq 단어 번역기 (tensorflow 버전)\n","\n","영어 단어를 한국어 단어로 번역"]},{"cell_type":"code","metadata":{"id":"4quT2Zx-57O3","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","import numpy as np"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UyJ00vwnBW1G","colab_type":"text"},"source":["S: 디코딩 입력의 시작\n","\n","E: 디코딩 출력의 끝\n","\n","P: 패딩, time step 크기보다 작은 단어의 경우, 빈 자리를 채우는 문자\n","* time step 4인 경우\n","  * love -> l, o, v, e\n","  * my -> m, y, P, P\n"]},{"cell_type":"markdown","metadata":{"id":"hXjkHsqffmBe","colab_type":"text"},"source":["## 데이터 준비"]},{"cell_type":"code","metadata":{"id":"zNbQzToa57S_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":791},"outputId":"b3efb4da-065b-4b5f-8e41-72dda0d2091d","executionInfo":{"status":"ok","timestamp":1567730714451,"user_tz":-540,"elapsed":491,"user":{"displayName":"Ji Un Hwang","photoUrl":"","userId":"04494318415747958635"}}},"source":["char_arr = [c for c in \"SEPabcdefghijklmnopqrstuvwxyz단어나무놀이소녀목록사랑\"]\n","char_arr    # corpus"],"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['S',\n"," 'E',\n"," 'P',\n"," 'a',\n"," 'b',\n"," 'c',\n"," 'd',\n"," 'e',\n"," 'f',\n"," 'g',\n"," 'h',\n"," 'i',\n"," 'j',\n"," 'k',\n"," 'l',\n"," 'm',\n"," 'n',\n"," 'o',\n"," 'p',\n"," 'q',\n"," 'r',\n"," 's',\n"," 't',\n"," 'u',\n"," 'v',\n"," 'w',\n"," 'x',\n"," 'y',\n"," 'z',\n"," '단',\n"," '어',\n"," '나',\n"," '무',\n"," '놀',\n"," '이',\n"," '소',\n"," '녀',\n"," '목',\n"," '록',\n"," '사',\n"," '랑']"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"PnUFFwXM57WJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":791},"outputId":"3cbd5fbd-10ce-40bd-9cba-27c14ffd0e93","executionInfo":{"status":"ok","timestamp":1567730886401,"user_tz":-540,"elapsed":506,"user":{"displayName":"Ji Un Hwang","photoUrl":"","userId":"04494318415747958635"}}},"source":["num_dic = {n:i for i,n in enumerate(char_arr)}\n","num_dic    # 딕셔너리"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'E': 1,\n"," 'P': 2,\n"," 'S': 0,\n"," 'a': 3,\n"," 'b': 4,\n"," 'c': 5,\n"," 'd': 6,\n"," 'e': 7,\n"," 'f': 8,\n"," 'g': 9,\n"," 'h': 10,\n"," 'i': 11,\n"," 'j': 12,\n"," 'k': 13,\n"," 'l': 14,\n"," 'm': 15,\n"," 'n': 16,\n"," 'o': 17,\n"," 'p': 18,\n"," 'q': 19,\n"," 'r': 20,\n"," 's': 21,\n"," 't': 22,\n"," 'u': 23,\n"," 'v': 24,\n"," 'w': 25,\n"," 'x': 26,\n"," 'y': 27,\n"," 'z': 28,\n"," '나': 31,\n"," '녀': 36,\n"," '놀': 33,\n"," '단': 29,\n"," '랑': 40,\n"," '록': 38,\n"," '목': 37,\n"," '무': 32,\n"," '사': 39,\n"," '소': 35,\n"," '어': 30,\n"," '이': 34}"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"9hO50Gi157Zl","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":36},"outputId":"33daf5b7-a8a4-4201-afcd-f6961b3d2a80","executionInfo":{"status":"ok","timestamp":1567730906150,"user_tz":-540,"elapsed":610,"user":{"displayName":"Ji Un Hwang","photoUrl":"","userId":"04494318415747958635"}}},"source":["dic_len = len(num_dic)    # 41\n","dic_len"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["41"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"HvC208ZS57dd","colab_type":"code","colab":{}},"source":["# 영어 단어를 한글로 번역하기 위한 학습용 데이터\n","\n","seq_data = [[\"word\", \"단어\"], [\"tree\", \"나무\"], [\"game\", \"놀이\"], [\"girl\", \"소녀\"], [\"list\", \"목록\"], [\"love\", \"사랑\"]]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lWijuf0ugK1N","colab_type":"text"},"source":["## 참고: np.eye() 로 원핫 인코딩"]},{"cell_type":"code","metadata":{"id":"PdKdiL3m57gf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":74},"outputId":"d983391d-273a-4c4a-ac97-2cc6ca5e30db","executionInfo":{"status":"ok","timestamp":1567738639214,"user_tz":-540,"elapsed":476,"user":{"displayName":"Ji Un Hwang","photoUrl":"","userId":"04494318415747958635"}}},"source":["np.eye(3)"],"execution_count":33,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[1., 0., 0.],\n","       [0., 1., 0.],\n","       [0., 0., 1.]])"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"code","metadata":{"id":"EFSDX_lF57jk","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":149},"outputId":"7bf650ec-a65d-4861-f1cc-7833bf681436","executionInfo":{"status":"ok","timestamp":1567738655551,"user_tz":-540,"elapsed":501,"user":{"displayName":"Ji Un Hwang","photoUrl":"","userId":"04494318415747958635"}}},"source":["np.eye(dic_len)    # 딕셔너리 길이만큼의 행이 생김 -> 딕셔너리 전체를 원핫인코딩 한것과 같은 형태"],"execution_count":34,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[1., 0., 0., ..., 0., 0., 0.],\n","       [0., 1., 0., ..., 0., 0., 0.],\n","       [0., 0., 1., ..., 0., 0., 0.],\n","       ...,\n","       [0., 0., 0., ..., 1., 0., 0.],\n","       [0., 0., 0., ..., 0., 1., 0.],\n","       [0., 0., 0., ..., 0., 0., 1.]])"]},"metadata":{"tags":[]},"execution_count":34}]},{"cell_type":"code","metadata":{"id":"Z0R02PBJ57mT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":187},"outputId":"b2c18ba9-9a66-4f9b-82dc-fa410aca78b9","executionInfo":{"status":"ok","timestamp":1567738717222,"user_tz":-540,"elapsed":596,"user":{"displayName":"Ji Un Hwang","photoUrl":"","userId":"04494318415747958635"}}},"source":["np.eye(dic_len)[[1, 5, 9]]    # np.eye()의 1, 5, 9번째 행 -> 딕셔너리의 1, 5, 9번째 인덱스넘버를 원핫 인코딩한 형태"],"execution_count":36,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0.]])"]},"metadata":{"tags":[]},"execution_count":36}]},{"cell_type":"markdown","metadata":{"id":"x4bD6BqKgEDD","colab_type":"text"},"source":["## 함수 만들기 1"]},{"cell_type":"code","metadata":{"id":"M009XYexLrPP","colab_type":"code","colab":{}},"source":["def make_batch(seq_data):    # (\"word\", \"PPPP\")\n","  input_batch = []\n","  output_batch = []\n","  target_batch = []\n","  for seq in seq_data:\n","    input = [num_dic[n] for n in seq[0]]    # 인코더 셀의 입력값\n","    output = [num_dic[n] for n in (\"S\" + seq[1])]    # 디코더 셀의 입력값    # \"S\"는 시작을 나타낸다\n","    target = [num_dic[n] for n in (seq[1] + \"E\")]    # 디코더 셀의 출력값    # \"E\"는 끝을 나타낸다\n","    input_batch.append(np.eye(dic_len)[input])    # 원핫 인코딩\n","    output_batch.append(np.eye(dic_len)[output])    # 원핫 인코딩\n","    target_batch.append(target)    # 출력값만 원핫 인코딩이 아니다\n","    # 출력값이 원핫인코딩 되어있지 않을 때 쓰는 softmax 함수 = sparse_softmax_cross_entropy_with_logits\n","  return input_batch, output_batch, target_batch"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oMo7fUovgCYc","colab_type":"text"},"source":["## 파라미터 설정"]},{"cell_type":"code","metadata":{"id":"4hrJiSSJQDCu","colab_type":"code","colab":{}},"source":["learning_rate = 0.01\n","n_hidden = 128\n","total_epoch = 100\n","# 입력과 출력의 형태는 원핫 인코딩으로 크기가 같다\n","n_class = n_input = dic_len"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C0nsoFRnfs01","colab_type":"text"},"source":["## 신경망 구성 및 학습\n"]},{"cell_type":"code","metadata":{"id":"VdB-wINMRDPO","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"a301145b-0838-4397-d2ba-7ae082fbaea5","executionInfo":{"status":"ok","timestamp":1567738391302,"user_tz":-540,"elapsed":1481,"user":{"displayName":"Ji Un Hwang","photoUrl":"","userId":"04494318415747958635"}}},"source":["# 신경망 구성\n","\n","tf.reset_default_graph()\n","\n","# seq2seq 모델은 인코더의 입력과 디코더의 입력의 형식이 같다: [batch size, time step, input size]\n","# 디코더의 출력 형식: [batch size, time step]\n","\n","enc_input = tf.placeholder(tf.float32, [None, None, n_input])    # batch size와 글자 길이(time step)가 정해져 있지 않다\n","# 같은 배치인 경우에는 입력 데이터의 글자수 (time step) 는 모두 같아야 한다\n","dec_input = tf.placeholder(tf.float32, [None, None, n_input])\n","targets = tf.placeholder(tf.int64, [None, None])\n","\n","with tf.variable_scope(\"encode\"):\n","  # 인코더 셀 구성\n","  enc_cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)\n","  # 오버피팅 방지하기 위해 드랍아웃\n","  enc_cell = tf.nn.rnn_cell.DropoutWrapper(enc_cell, output_keep_prob=0.5)\n","  # 드라이버 설정\n","  outputs, enc_states = tf.nn.dynamic_rnn(enc_cell, enc_input, dtype=tf.float32)\n","\n","with tf.variable_scope(\"decode\"):\n","  # 디코더 셀 구성\n","  dec_cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)\n","  dec_cell = tf.nn.rnn_cell.DropoutWrapper(dec_cell, output_keep_prob=0.5)\n","  # 드라이버 설정\n","  outputs, dec_states = tf.nn.dynamic_rnn(dec_cell, dec_input, initial_state=enc_states, dtype=tf.float32)\n","  # initial_state=enc_states: seq2seq 모델은 인코더 셀의 최종 상태 값을 디코더 셀의 초기 상태값으로 입력해줘야 한다\n","\n","model = tf.layers.dense(outputs, n_class, activation=None)\n","# model 수행 결과가 [batch size, time step, input]\n","cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=model, labels=targets))\n","optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n","\n","\n","\n","# 학습\n","\n","sess = tf.Session()\n","sess.run(tf.global_variables_initializer())\n","\n","input_batch, output_batch, target_batch = make_batch(seq_data)\n","\n","for epoch in range(total_epoch):\n","  _, loss = sess.run([optimizer, cost], feed_dict={enc_input:input_batch, dec_input: output_batch, targets: target_batch})\n","  print(\"epoch: {:04d}, cost: {:.6f}\".format(epoch+1, loss))\n","print(\"모델 완료\")"],"execution_count":30,"outputs":[{"output_type":"stream","text":["epoch: 0001, cost: 3.786503\n","epoch: 0002, cost: 2.797024\n","epoch: 0003, cost: 1.622213\n","epoch: 0004, cost: 0.990022\n","epoch: 0005, cost: 0.603112\n","epoch: 0006, cost: 0.285041\n","epoch: 0007, cost: 0.148995\n","epoch: 0008, cost: 0.089851\n","epoch: 0009, cost: 0.047263\n","epoch: 0010, cost: 0.060495\n","epoch: 0011, cost: 0.029462\n","epoch: 0012, cost: 0.036320\n","epoch: 0013, cost: 0.034499\n","epoch: 0014, cost: 0.015579\n","epoch: 0015, cost: 0.005542\n","epoch: 0016, cost: 0.009919\n","epoch: 0017, cost: 0.005139\n","epoch: 0018, cost: 0.003253\n","epoch: 0019, cost: 0.005737\n","epoch: 0020, cost: 0.005223\n","epoch: 0021, cost: 0.005035\n","epoch: 0022, cost: 0.001811\n","epoch: 0023, cost: 0.001322\n","epoch: 0024, cost: 0.004328\n","epoch: 0025, cost: 0.003073\n","epoch: 0026, cost: 0.001941\n","epoch: 0027, cost: 0.008183\n","epoch: 0028, cost: 0.000624\n","epoch: 0029, cost: 0.010115\n","epoch: 0030, cost: 0.000757\n","epoch: 0031, cost: 0.007256\n","epoch: 0032, cost: 0.000576\n","epoch: 0033, cost: 0.001436\n","epoch: 0034, cost: 0.000331\n","epoch: 0035, cost: 0.001294\n","epoch: 0036, cost: 0.000501\n","epoch: 0037, cost: 0.000940\n","epoch: 0038, cost: 0.001493\n","epoch: 0039, cost: 0.000668\n","epoch: 0040, cost: 0.000311\n","epoch: 0041, cost: 0.000737\n","epoch: 0042, cost: 0.002407\n","epoch: 0043, cost: 0.000366\n","epoch: 0044, cost: 0.000464\n","epoch: 0045, cost: 0.002000\n","epoch: 0046, cost: 0.000423\n","epoch: 0047, cost: 0.000480\n","epoch: 0048, cost: 0.001116\n","epoch: 0049, cost: 0.000409\n","epoch: 0050, cost: 0.000227\n","epoch: 0051, cost: 0.000251\n","epoch: 0052, cost: 0.000574\n","epoch: 0053, cost: 0.001096\n","epoch: 0054, cost: 0.001332\n","epoch: 0055, cost: 0.000717\n","epoch: 0056, cost: 0.000686\n","epoch: 0057, cost: 0.000141\n","epoch: 0058, cost: 0.000289\n","epoch: 0059, cost: 0.000106\n","epoch: 0060, cost: 0.000949\n","epoch: 0061, cost: 0.000359\n","epoch: 0062, cost: 0.000348\n","epoch: 0063, cost: 0.000435\n","epoch: 0064, cost: 0.000289\n","epoch: 0065, cost: 0.000173\n","epoch: 0066, cost: 0.000361\n","epoch: 0067, cost: 0.000293\n","epoch: 0068, cost: 0.000845\n","epoch: 0069, cost: 0.000339\n","epoch: 0070, cost: 0.000216\n","epoch: 0071, cost: 0.000207\n","epoch: 0072, cost: 0.000975\n","epoch: 0073, cost: 0.000198\n","epoch: 0074, cost: 0.000279\n","epoch: 0075, cost: 0.000459\n","epoch: 0076, cost: 0.001071\n","epoch: 0077, cost: 0.000179\n","epoch: 0078, cost: 0.000138\n","epoch: 0079, cost: 0.000089\n","epoch: 0080, cost: 0.000511\n","epoch: 0081, cost: 0.000215\n","epoch: 0082, cost: 0.000466\n","epoch: 0083, cost: 0.000442\n","epoch: 0084, cost: 0.000229\n","epoch: 0085, cost: 0.000100\n","epoch: 0086, cost: 0.000189\n","epoch: 0087, cost: 0.001146\n","epoch: 0088, cost: 0.000163\n","epoch: 0089, cost: 0.000154\n","epoch: 0090, cost: 0.000196\n","epoch: 0091, cost: 0.000241\n","epoch: 0092, cost: 0.000397\n","epoch: 0093, cost: 0.000572\n","epoch: 0094, cost: 0.000111\n","epoch: 0095, cost: 0.000220\n","epoch: 0096, cost: 0.000110\n","epoch: 0097, cost: 0.000227\n","epoch: 0098, cost: 0.000566\n","epoch: 0099, cost: 0.000614\n","epoch: 0100, cost: 0.000345\n","모델 완료\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pxonn584gGfW","colab_type":"text"},"source":["## 함수 만들기 2"]},{"cell_type":"code","metadata":{"id":"UXFPVYrQ57p1","colab_type":"code","colab":{}},"source":["# 단어를 입력받아서 번역된 단어를 예측하고 디코딩하는 함수\n","\n","def translate(word):\n","  # 단어 번역\n","  # [영어 단어, 한글 단어]\n","  # 예측시에는 한글단어를 모르므로, 디코더의 입출력값을 의미없는 값인 P로 채운다: ['word', 'PPPP']\n","  seq_data = [word, \"P\"*len(word)]\n","  input_batch, output_batch, target_batch = make_batch([seq_data])\n","  # [batch size, time step, input]\n","  prediction = tf.argmax(model, 2)    # model 수행 결과가 [batch size, time step, input]\n","  # 2번째 차원인 input 차원을 argmax로 해서 가장 확률이 높은 글자를 예측값으로 한다.\n","  result = sess.run(prediction, feed_dict={enc_input:input_batch, dec_input: output_batch, targets: target_batch})\n","  decoded = [char_arr[i] for i in result[0]]    # 결과값이 숫자로 나오니까 숫자의 인덱스에 해당하는 문자를 가져온 것\n","  # 출력의 끝에 해당하는 \"E\" 이후의 글자들을 제거하고 문자열을 출력\n","  end = decoded.index(\"E\")    # \"E\"라는 글자의 인덱스 넘버를 가져와서 end에 넣어라\n","  translated = \"\".join(decoded[:end])\n","  return translated"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nTF1_FEgf8gN","colab_type":"text"},"source":["## 번역"]},{"cell_type":"code","metadata":{"id":"OrTAxOPM57s0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":111},"outputId":"2976f0d4-dd52-4dc2-e875-842308835d01","executionInfo":{"status":"ok","timestamp":1567738401372,"user_tz":-540,"elapsed":716,"user":{"displayName":"Ji Un Hwang","photoUrl":"","userId":"04494318415747958635"}}},"source":["print(\"word -> \", translate(\"word\"))    # 단어\n","print(\"wodr -> \", translate(\"wodr\"))    # 단어\n","print(\"love -> \", translate(\"love\"))    # 사랑\n","print(\"loev -> \", translate(\"loev\"))    # 사랑\n","print(\"abcd -> \", translate(\"abcd\"))    # ????"],"execution_count":32,"outputs":[{"output_type":"stream","text":["word ->  단어\n","wodr ->  단어\n","love ->  사랑\n","loev ->  사랑\n","abcd ->  무사랑\n"],"name":"stdout"}]}]}
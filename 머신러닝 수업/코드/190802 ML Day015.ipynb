{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예제: 전처리(표준화)를 하지 않아서 발산하는 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy = np.array([[828.659973, 833.450012, 908100, 828.349976, 831.659973],\n",
    "               [823.02002, 828.070007, 1828100, 821.655029, 828.070007],\n",
    "               [819.929993, 824.400024, 1438100, 818.97998, 824.159973],\n",
    "               [816, 820.958984, 1008100, 815.48999, 819.23999],\n",
    "               [819.359985, 823, 1188100, 818.469971, 818.97998],\n",
    "               [819, 823, 1198100, 816, 820.450012],\n",
    "               [811.700012, 815.25, 1098100, 809.780029, 813.669983],\n",
    "               [809.51001, 816.659973, 1398100, 804.539978, 809.559998]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 5)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 1)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xdata = xy[:, 0:-1]\n",
    "ydata = xy[:, [-1]]\n",
    "ydata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, 4])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "w = tf.Variable(tf.random_normal([4, 1]), name=\"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]), name=\"bias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf = tf.matmul(x, w) + b\n",
    "cost = tf.reduce_mean(tf.square(hf - y))\n",
    "train = tf.train.GradientDescentOptimizer(1e-10).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:  0 , cost:  3343368000000.0 hf:  [[1291185.9]\n",
      " [2596678.2]\n",
      " [2043244.9]\n",
      " [1433050.5]\n",
      " [1688483.1]\n",
      " [1702674.6]\n",
      " [1560747.2]\n",
      " [1986461.2]]\n",
      "step:  1 , cost:  3.651163e+17 hf:  [[-4.2623146e+08]\n",
      " [-8.5805024e+08]\n",
      " [-6.7499667e+08]\n",
      " [-4.7316829e+08]\n",
      " [-5.5765459e+08]\n",
      " [-5.6234829e+08]\n",
      " [-5.1541146e+08]\n",
      " [-6.5622189e+08]]\n",
      "step:  2 , cost:  3.987294e+22 hf:  [[1.4085439e+11]\n",
      " [2.8355435e+11]\n",
      " [2.2306198e+11]\n",
      " [1.5636526e+11]\n",
      " [1.8428482e+11]\n",
      " [1.8583591e+11]\n",
      " [1.7032505e+11]\n",
      " [2.1685764e+11]]\n",
      "step:  3 , cost:  4.3543702e+27 hf:  [[-4.6547245e+13]\n",
      " [-9.3704375e+13]\n",
      " [-7.3713853e+13]\n",
      " [-5.1673016e+13]\n",
      " [-6.0899415e+13]\n",
      " [-6.1411993e+13]\n",
      " [-5.6286213e+13]\n",
      " [-7.1663543e+13]]\n",
      "step:  4 , cost:  4.7552387e+32 hf:  [[1.5382166e+16]\n",
      " [3.0965877e+16]\n",
      " [2.4359738e+16]\n",
      " [1.7076047e+16]\n",
      " [2.0125033e+16]\n",
      " [2.0294423e+16]\n",
      " [1.8600539e+16]\n",
      " [2.3682186e+16]]\n",
      "step:  5 , cost:  inf hf:  [[-5.0832446e+18]\n",
      " [-1.0233093e+19]\n",
      " [-8.0500046e+18]\n",
      " [-5.6430103e+18]\n",
      " [-6.6505890e+18]\n",
      " [-6.7065657e+18]\n",
      " [-6.1468000e+18]\n",
      " [-7.8260978e+18]]\n",
      "step:  6 , cost:  inf hf:  [[1.6798269e+21]\n",
      " [3.3816637e+21]\n",
      " [2.6602330e+21]\n",
      " [1.8648090e+21]\n",
      " [2.1977772e+21]\n",
      " [2.2162754e+21]\n",
      " [2.0312932e+21]\n",
      " [2.5862399e+21]]\n",
      "step:  7 , cost:  inf hf:  [[-5.5512151e+23]\n",
      " [-1.1175165e+24]\n",
      " [-8.7910993e+23]\n",
      " [-6.1625139e+23]\n",
      " [-7.2628521e+23]\n",
      " [-7.3239814e+23]\n",
      " [-6.7126830e+23]\n",
      " [-8.5465797e+23]]\n",
      "step:  8 , cost:  inf hf:  [[1.8344739e+26]\n",
      " [3.6929843e+26]\n",
      " [2.9051375e+26]\n",
      " [2.0364861e+26]\n",
      " [2.4001075e+26]\n",
      " [2.4203087e+26]\n",
      " [2.2182966e+26]\n",
      " [2.8243325e+26]]\n",
      "step:  9 , cost:  inf hf:  [[-6.0622671e+28]\n",
      " [-1.2203965e+29]\n",
      " [-9.6004199e+28]\n",
      " [-6.7298426e+28]\n",
      " [-7.9314799e+28]\n",
      " [-7.9982372e+28]\n",
      " [-7.3306612e+28]\n",
      " [-9.3333890e+28]]\n",
      "step:  10 , cost:  inf hf:  [[2.0033582e+31]\n",
      " [4.0329657e+31]\n",
      " [3.1725886e+31]\n",
      " [2.2239677e+31]\n",
      " [2.6210648e+31]\n",
      " [2.6431258e+31]\n",
      " [2.4225164e+31]\n",
      " [3.0843448e+31]]\n",
      "step:  11 , cost:  inf hf:  [[-6.62036839e+33]\n",
      " [-1.33274805e+34]\n",
      " [-1.04842479e+34]\n",
      " [-7.34940181e+33]\n",
      " [-8.66166343e+33]\n",
      " [-8.73456634e+33]\n",
      " [-8.00553231e+33]\n",
      " [-1.01926344e+34]]\n",
      "step:  12 , cost:  inf hf:  [[inf]\n",
      " [inf]\n",
      " [inf]\n",
      " [inf]\n",
      " [inf]\n",
      " [inf]\n",
      " [inf]\n",
      " [inf]]\n",
      "step:  13 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "step:  14 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "step:  15 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "step:  16 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "step:  17 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "step:  18 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "step:  19 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "step:  20 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "step:  21 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "step:  22 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "step:  23 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "step:  24 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "step:  25 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "step:  26 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "step:  27 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "step:  28 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "step:  29 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "step:  30 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "step:  31 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "step:  32 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "step:  33 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "step:  34 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "step:  35 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "step:  36 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "step:  37 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "step:  38 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "step:  39 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "step:  40 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "step:  41 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "step:  42 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "step:  43 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "step:  44 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "step:  45 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "step:  46 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "step:  47 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "step:  48 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "step:  49 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "step:  50 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "step:  51 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "step:  52 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "step:  53 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "step:  54 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "step:  55 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "step:  56 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "step:  57 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "step:  58 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "step:  59 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "step:  60 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "step:  61 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "step:  62 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "step:  63 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "step:  64 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "step:  65 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "step:  66 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "step:  67 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "step:  68 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "step:  69 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "step:  70 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "step:  71 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "step:  72 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "step:  73 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "step:  74 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "step:  75 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "step:  76 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "step:  77 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "step:  78 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "step:  79 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "step:  80 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "step:  81 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "step:  82 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "step:  83 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "step:  84 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "step:  85 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "step:  86 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "step:  87 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "step:  88 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "step:  89 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "step:  90 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "step:  91 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "step:  92 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "step:  93 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "step:  94 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "step:  95 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "step:  96 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "step:  97 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "step:  98 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "step:  99 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "step:  100 , cost:  nan hf:  [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n"
     ]
    }
   ],
   "source": [
    "for step in range(101):\n",
    "    cv, hv, _ = sess.run([cost, hf, train], feed_dict={x:xdata, y:ydata})\n",
    "    print(\"step: \", step, \", cost: \", cv, \"hf: \", hv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예제: 전처리(Scaling)을 한 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy = np.array([[828.659973, 833.450012, 908100, 828.349976, 831.659973],\n",
    "               [823.02002, 828.070007, 1828100, 821.655029, 828.070007],\n",
    "               [819.929993, 824.400024, 1438100, 818.97998, 824.159973],\n",
    "               [816, 820.958984, 1008100, 815.48999, 819.23999],\n",
    "               [819.359985, 823, 1188100, 818.469971, 818.97998],\n",
    "               [819, 823, 1198100, 816, 820.450012],\n",
    "               [811.700012, 815.25, 1098100, 809.780029, 813.669983],\n",
    "               [809.51001, 816.659973, 1398100, 804.539978, 809.559998]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdata = xy[:, 0:-1]\n",
    "ydata = xy[:, [-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MyScaler(data):    # MinMaxScaling을 하는 함수\n",
    "#     print(np.min(data))    # 전체 값 중에 최소값 출력\n",
    "#     print(np.min(data, axis=1))    # 행 단위 최소값\n",
    "#     print(np.min(data, axis=0))    # 열 단위 최소값\n",
    "    de = np.max(data, axis=0) - np.min(data, axis=0)\n",
    "    num = data - np.min(data, axis=0)\n",
    "    return num / de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 1.        , 0.        , 1.        ],\n",
       "       [0.70548491, 0.70439552, 1.        , 0.71881783],\n",
       "       [0.54412549, 0.50274824, 0.57608696, 0.60646801],\n",
       "       [0.33890353, 0.31368023, 0.10869565, 0.45989134],\n",
       "       [0.51436   , 0.4258239 , 0.30434783, 0.58504805],\n",
       "       [0.49556179, 0.4258239 , 0.31521739, 0.48131134],\n",
       "       [0.11436064, 0.        , 0.20652174, 0.22007776],\n",
       "       [0.        , 0.07747099, 0.5326087 , 0.        ]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xdata = MyScaler(xdata)    # x값만 표준화하면 된다. y값도 표준화할 경우 다 끝난 후에 값을 복원해줘야 한다.\n",
    "xdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, 4])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "w = tf.Variable(tf.random_normal([4, 1]), name=\"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]), name=\"bias\")\n",
    "\n",
    "hf = tf.matmul(x, w) + b\n",
    "cost = tf.reduce_mean(tf.square(hf - y))\n",
    "train = tf.train.GradientDescentOptimizer(1e-6).minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:  0 , cost:  672851.5 hf:  [[ 3.1606014 ]\n",
      " [ 1.3391173 ]\n",
      " [ 0.7761264 ]\n",
      " [ 0.1203649 ]\n",
      " [ 0.6987512 ]\n",
      " [ 0.49894953]\n",
      " [-1.1438227 ]\n",
      " [-1.7187885 ]]\n",
      "step:  1 , cost:  672846.6 hf:  [[ 3.164555  ]\n",
      " [ 1.3430245 ]\n",
      " [ 0.779408  ]\n",
      " [ 0.12294066]\n",
      " [ 0.7017679 ]\n",
      " [ 0.5018718 ]\n",
      " [-1.1417813 ]\n",
      " [-1.7167602 ]]\n",
      "step:  2 , cost:  672841.75 hf:  [[ 3.1685083 ]\n",
      " [ 1.3469317 ]\n",
      " [ 0.7826898 ]\n",
      " [ 0.12551641]\n",
      " [ 0.70478463]\n",
      " [ 0.5047939 ]\n",
      " [-1.13974   ]\n",
      " [-1.714732  ]]\n",
      "step:  3 , cost:  672836.8 hf:  [[ 3.1724615 ]\n",
      " [ 1.3508389 ]\n",
      " [ 0.7859714 ]\n",
      " [ 0.12809205]\n",
      " [ 0.70780134]\n",
      " [ 0.50771594]\n",
      " [-1.1376987 ]\n",
      " [-1.7127038 ]]\n",
      "step:  4 , cost:  672832.0 hf:  [[ 3.176415  ]\n",
      " [ 1.354746  ]\n",
      " [ 0.7892529 ]\n",
      " [ 0.1306678 ]\n",
      " [ 0.71081793]\n",
      " [ 0.5106379 ]\n",
      " [-1.1356573 ]\n",
      " [-1.7106757 ]]\n",
      "step:  5 , cost:  672827.1 hf:  [[ 3.1803687 ]\n",
      " [ 1.3586533 ]\n",
      " [ 0.7925346 ]\n",
      " [ 0.13324332]\n",
      " [ 0.7138343 ]\n",
      " [ 0.5135598 ]\n",
      " [-1.1336161 ]\n",
      " [-1.7086476 ]]\n",
      "step:  6 , cost:  672822.25 hf:  [[ 3.1843219 ]\n",
      " [ 1.3625602 ]\n",
      " [ 0.79581606]\n",
      " [ 0.13581908]\n",
      " [ 0.71685064]\n",
      " [ 0.51648176]\n",
      " [-1.1315749 ]\n",
      " [-1.7066195 ]]\n",
      "step:  7 , cost:  672817.4 hf:  [[ 3.1882749]\n",
      " [ 1.366467 ]\n",
      " [ 0.7990973]\n",
      " [ 0.1383946]\n",
      " [ 0.7198672]\n",
      " [ 0.5194037]\n",
      " [-1.1295336]\n",
      " [-1.7045915]]\n",
      "step:  8 , cost:  672812.5 hf:  [[ 3.1922278 ]\n",
      " [ 1.3703738 ]\n",
      " [ 0.8023788 ]\n",
      " [ 0.14096999]\n",
      " [ 0.7228836 ]\n",
      " [ 0.52232563]\n",
      " [-1.1274924 ]\n",
      " [-1.7025634 ]]\n",
      "step:  9 , cost:  672807.6 hf:  [[ 3.196181  ]\n",
      " [ 1.3742809 ]\n",
      " [ 0.80566025]\n",
      " [ 0.14354563]\n",
      " [ 0.7258997 ]\n",
      " [ 0.5252476 ]\n",
      " [-1.1254512 ]\n",
      " [-1.7005353 ]]\n",
      "step:  10 , cost:  672802.7 hf:  [[ 3.2001343 ]\n",
      " [ 1.3781875 ]\n",
      " [ 0.8089417 ]\n",
      " [ 0.14612114]\n",
      " [ 0.72891605]\n",
      " [ 0.5281695 ]\n",
      " [-1.12341   ]\n",
      " [-1.6985072 ]]\n",
      "step:  11 , cost:  672797.9 hf:  [[ 3.2040873 ]\n",
      " [ 1.3820944 ]\n",
      " [ 0.81222296]\n",
      " [ 0.14869666]\n",
      " [ 0.73193264]\n",
      " [ 0.5310912 ]\n",
      " [-1.1213688 ]\n",
      " [-1.6964791 ]]\n",
      "step:  12 , cost:  672793.0 hf:  [[ 3.2080402 ]\n",
      " [ 1.3860012 ]\n",
      " [ 0.81550467]\n",
      " [ 0.15127218]\n",
      " [ 0.73494875]\n",
      " [ 0.53401315]\n",
      " [-1.1193277 ]\n",
      " [-1.6944511 ]]\n",
      "step:  13 , cost:  672788.1 hf:  [[ 3.2119935 ]\n",
      " [ 1.3899081 ]\n",
      " [ 0.8187859 ]\n",
      " [ 0.1538477 ]\n",
      " [ 0.73796535]\n",
      " [ 0.5369351 ]\n",
      " [-1.1172864 ]\n",
      " [-1.692423  ]]\n",
      "step:  14 , cost:  672783.25 hf:  [[ 3.2159467 ]\n",
      " [ 1.3938149 ]\n",
      " [ 0.8220674 ]\n",
      " [ 0.15642321]\n",
      " [ 0.7409817 ]\n",
      " [ 0.5398568 ]\n",
      " [-1.1152452 ]\n",
      " [-1.6903949 ]]\n",
      "step:  15 , cost:  672778.4 hf:  [[ 3.2198997 ]\n",
      " [ 1.3977218 ]\n",
      " [ 0.8253486 ]\n",
      " [ 0.15899885]\n",
      " [ 0.7439978 ]\n",
      " [ 0.54277873]\n",
      " [-1.113204  ]\n",
      " [-1.6883668 ]]\n",
      "step:  16 , cost:  672773.5 hf:  [[ 3.2238526 ]\n",
      " [ 1.4016286 ]\n",
      " [ 0.8286301 ]\n",
      " [ 0.16157424]\n",
      " [ 0.74701416]\n",
      " [ 0.54570067]\n",
      " [-1.1111628 ]\n",
      " [-1.6863388 ]]\n",
      "step:  17 , cost:  672768.6 hf:  [[ 3.2278059 ]\n",
      " [ 1.4055355 ]\n",
      " [ 0.8319113 ]\n",
      " [ 0.16414988]\n",
      " [ 0.75003076]\n",
      " [ 0.54862237]\n",
      " [-1.1091216 ]\n",
      " [-1.6843107 ]]\n",
      "step:  18 , cost:  672763.75 hf:  [[ 3.231759  ]\n",
      " [ 1.4094423 ]\n",
      " [ 0.8351928 ]\n",
      " [ 0.16672528]\n",
      " [ 0.7530469 ]\n",
      " [ 0.5515443 ]\n",
      " [-1.1070805 ]\n",
      " [-1.6822826 ]]\n",
      "step:  19 , cost:  672758.9 hf:  [[ 3.235712  ]\n",
      " [ 1.4133492 ]\n",
      " [ 0.8384743 ]\n",
      " [ 0.1693008 ]\n",
      " [ 0.75606346]\n",
      " [ 0.55446625]\n",
      " [-1.1050392 ]\n",
      " [-1.6802546 ]]\n",
      "step:  20 , cost:  672754.0 hf:  [[ 3.239665  ]\n",
      " [ 1.417256  ]\n",
      " [ 0.8417555 ]\n",
      " [ 0.17187655]\n",
      " [ 0.7590798 ]\n",
      " [ 0.55738795]\n",
      " [-1.102998  ]\n",
      " [-1.6782265 ]]\n",
      "step:  21 , cost:  672749.2 hf:  [[ 3.2436182 ]\n",
      " [ 1.4211628 ]\n",
      " [ 0.8450372 ]\n",
      " [ 0.17445183]\n",
      " [ 0.76209617]\n",
      " [ 0.5603099 ]\n",
      " [-1.1009568 ]\n",
      " [-1.6761984 ]]\n",
      "step:  22 , cost:  672744.3 hf:  [[ 3.2475715 ]\n",
      " [ 1.4250697 ]\n",
      " [ 0.8483182 ]\n",
      " [ 0.17702758]\n",
      " [ 0.7651123 ]\n",
      " [ 0.5632316 ]\n",
      " [-1.0989156 ]\n",
      " [-1.6741704 ]]\n",
      "step:  23 , cost:  672739.44 hf:  [[ 3.2515244 ]\n",
      " [ 1.4289765 ]\n",
      " [ 0.8515997 ]\n",
      " [ 0.1796031 ]\n",
      " [ 0.76812863]\n",
      " [ 0.5661535 ]\n",
      " [-1.0968745 ]\n",
      " [-1.6721423 ]]\n",
      "step:  24 , cost:  672734.56 hf:  [[ 3.2554774 ]\n",
      " [ 1.4328833 ]\n",
      " [ 0.85488105]\n",
      " [ 0.18217838]\n",
      " [ 0.77114487]\n",
      " [ 0.56907535]\n",
      " [-1.0948334 ]\n",
      " [-1.6701143 ]]\n",
      "step:  25 , cost:  672729.7 hf:  [[ 3.2594304 ]\n",
      " [ 1.43679   ]\n",
      " [ 0.85816216]\n",
      " [ 0.1847539 ]\n",
      " [ 0.77416134]\n",
      " [ 0.57199717]\n",
      " [-1.0927923 ]\n",
      " [-1.6680864 ]]\n",
      "step:  26 , cost:  672724.8 hf:  [[ 3.2633834 ]\n",
      " [ 1.4406965 ]\n",
      " [ 0.8614435 ]\n",
      " [ 0.18732917]\n",
      " [ 0.77717733]\n",
      " [ 0.57491875]\n",
      " [-1.0907512 ]\n",
      " [-1.6660584 ]]\n",
      "step:  27 , cost:  672719.94 hf:  [[ 3.2673364 ]\n",
      " [ 1.4446032 ]\n",
      " [ 0.8647249 ]\n",
      " [ 0.18990457]\n",
      " [ 0.78019357]\n",
      " [ 0.57784057]\n",
      " [-1.0887101 ]\n",
      " [-1.6640304 ]]\n",
      "step:  28 , cost:  672715.06 hf:  [[ 3.2712893 ]\n",
      " [ 1.4485099 ]\n",
      " [ 0.868006  ]\n",
      " [ 0.19248009]\n",
      " [ 0.7832098 ]\n",
      " [ 0.58076215]\n",
      " [-1.086669  ]\n",
      " [-1.6620026 ]]\n",
      "step:  29 , cost:  672710.2 hf:  [[ 3.2752423 ]\n",
      " [ 1.4524169 ]\n",
      " [ 0.87128735]\n",
      " [ 0.19505548]\n",
      " [ 0.78622603]\n",
      " [ 0.5836837 ]\n",
      " [-1.084628  ]\n",
      " [-1.6599746 ]]\n",
      "step:  30 , cost:  672705.4 hf:  [[ 3.2791953 ]\n",
      " [ 1.4563234 ]\n",
      " [ 0.87456846]\n",
      " [ 0.19763064]\n",
      " [ 0.78924227]\n",
      " [ 0.5866058 ]\n",
      " [-1.0825869 ]\n",
      " [-1.6579467 ]]\n",
      "step:  31 , cost:  672700.5 hf:  [[ 3.2831483 ]\n",
      " [ 1.4602301 ]\n",
      " [ 0.8778498 ]\n",
      " [ 0.20020628]\n",
      " [ 0.7922585 ]\n",
      " [ 0.58952737]\n",
      " [-1.0805459 ]\n",
      " [-1.6559187 ]]\n",
      "step:  32 , cost:  672695.6 hf:  [[ 3.2871013 ]\n",
      " [ 1.4641366 ]\n",
      " [ 0.8811312 ]\n",
      " [ 0.20278156]\n",
      " [ 0.79527473]\n",
      " [ 0.59244895]\n",
      " [-1.0785048 ]\n",
      " [-1.6538908 ]]\n",
      "step:  33 , cost:  672690.75 hf:  [[ 3.2910542 ]\n",
      " [ 1.4680433 ]\n",
      " [ 0.8844123 ]\n",
      " [ 0.2053572 ]\n",
      " [ 0.79829097]\n",
      " [ 0.59537077]\n",
      " [-1.0764637 ]\n",
      " [-1.6518629 ]]\n",
      "step:  34 , cost:  672685.8 hf:  [[ 3.2950072 ]\n",
      " [ 1.47195   ]\n",
      " [ 0.8876934 ]\n",
      " [ 0.20793247]\n",
      " [ 0.8013072 ]\n",
      " [ 0.5982926 ]\n",
      " [-1.0744226 ]\n",
      " [-1.649835  ]]\n",
      "step:  35 , cost:  672681.0 hf:  [[ 3.2989602 ]\n",
      " [ 1.4758568 ]\n",
      " [ 0.89097476]\n",
      " [ 0.21050775]\n",
      " [ 0.80432343]\n",
      " [ 0.6012144 ]\n",
      " [-1.0723815 ]\n",
      " [-1.647807  ]]\n",
      "step:  36 , cost:  672676.1 hf:  [[ 3.3029132 ]\n",
      " [ 1.4797635 ]\n",
      " [ 0.8942561 ]\n",
      " [ 0.21308327]\n",
      " [ 0.80733967]\n",
      " [ 0.604136  ]\n",
      " [-1.0703405 ]\n",
      " [-1.6457791 ]]\n",
      "step:  37 , cost:  672671.25 hf:  [[ 3.3068662 ]\n",
      " [ 1.4836702 ]\n",
      " [ 0.89753723]\n",
      " [ 0.21565866]\n",
      " [ 0.8103559 ]\n",
      " [ 0.6070576 ]\n",
      " [-1.0682994 ]\n",
      " [-1.6437511 ]]\n",
      "step:  38 , cost:  672666.4 hf:  [[ 3.3108191 ]\n",
      " [ 1.487577  ]\n",
      " [ 0.9008186 ]\n",
      " [ 0.21823406]\n",
      " [ 0.81337214]\n",
      " [ 0.6099794 ]\n",
      " [-1.0662583 ]\n",
      " [-1.6417233 ]]\n",
      "step:  39 , cost:  672661.5 hf:  [[ 3.3147721 ]\n",
      " [ 1.4914834 ]\n",
      " [ 0.90409994]\n",
      " [ 0.22080946]\n",
      " [ 0.81638813]\n",
      " [ 0.6129012 ]\n",
      " [-1.0642173 ]\n",
      " [-1.6396953 ]]\n",
      "step:  40 , cost:  672656.6 hf:  [[ 3.318725  ]\n",
      " [ 1.4953902 ]\n",
      " [ 0.90738106]\n",
      " [ 0.22338486]\n",
      " [ 0.81940436]\n",
      " [ 0.6158228 ]\n",
      " [-1.0621762 ]\n",
      " [-1.6376674 ]]\n",
      "step:  41 , cost:  672651.75 hf:  [[ 3.322678  ]\n",
      " [ 1.4992969 ]\n",
      " [ 0.9106624 ]\n",
      " [ 0.22596025]\n",
      " [ 0.8224206 ]\n",
      " [ 0.6187446 ]\n",
      " [-1.0601351 ]\n",
      " [-1.6356394 ]]\n",
      "step:  42 , cost:  672646.9 hf:  [[ 3.326631  ]\n",
      " [ 1.5032034 ]\n",
      " [ 0.9139433 ]\n",
      " [ 0.22853565]\n",
      " [ 0.82543683]\n",
      " [ 0.62166643]\n",
      " [-1.058094  ]\n",
      " [-1.6336116 ]]\n",
      "step:  43 , cost:  672642.0 hf:  [[ 3.3305836 ]\n",
      " [ 1.5071099 ]\n",
      " [ 0.91722465]\n",
      " [ 0.23111105]\n",
      " [ 0.82845306]\n",
      " [ 0.624588  ]\n",
      " [-1.056053  ]\n",
      " [-1.6315837 ]]\n",
      "step:  44 , cost:  672637.1 hf:  [[ 3.3345366 ]\n",
      " [ 1.5110158 ]\n",
      " [ 0.9205054 ]\n",
      " [ 0.23368609]\n",
      " [ 0.83146894]\n",
      " [ 0.6275095 ]\n",
      " [-1.0540121 ]\n",
      " [-1.6295558 ]]\n",
      "step:  45 , cost:  672632.25 hf:  [[ 3.3384888 ]\n",
      " [ 1.5149224 ]\n",
      " [ 0.9237864 ]\n",
      " [ 0.23626125]\n",
      " [ 0.8344848 ]\n",
      " [ 0.63043094]\n",
      " [-1.0519712 ]\n",
      " [-1.6275281 ]]\n",
      "step:  46 , cost:  672627.4 hf:  [[ 3.342441  ]\n",
      " [ 1.5188285 ]\n",
      " [ 0.9270674 ]\n",
      " [ 0.23883641]\n",
      " [ 0.8375007 ]\n",
      " [ 0.6333524 ]\n",
      " [-1.0499303 ]\n",
      " [-1.6255002 ]]\n",
      "step:  47 , cost:  672622.5 hf:  [[ 3.346394  ]\n",
      " [ 1.5227351 ]\n",
      " [ 0.9303484 ]\n",
      " [ 0.24141145]\n",
      " [ 0.84051657]\n",
      " [ 0.6362736 ]\n",
      " [-1.0478895 ]\n",
      " [-1.6234725 ]]\n",
      "step:  48 , cost:  672617.6 hf:  [[ 3.3503466 ]\n",
      " [ 1.5266412 ]\n",
      " [ 0.93362916]\n",
      " [ 0.24398673]\n",
      " [ 0.84353244]\n",
      " [ 0.6391951 ]\n",
      " [-1.0458485 ]\n",
      " [-1.6214447 ]]\n",
      "step:  49 , cost:  672612.75 hf:  [[ 3.3542988 ]\n",
      " [ 1.5305476 ]\n",
      " [ 0.9369099 ]\n",
      " [ 0.24656177]\n",
      " [ 0.8465483 ]\n",
      " [ 0.64211655]\n",
      " [-1.0438075 ]\n",
      " [-1.619417  ]]\n",
      "step:  50 , cost:  672607.9 hf:  [[ 3.358251  ]\n",
      " [ 1.534454  ]\n",
      " [ 0.9401909 ]\n",
      " [ 0.24913692]\n",
      " [ 0.8495642 ]\n",
      " [ 0.645038  ]\n",
      " [-1.0417666 ]\n",
      " [-1.6173891 ]]\n",
      "step:  51 , cost:  672603.0 hf:  [[ 3.3622036 ]\n",
      " [ 1.5383601 ]\n",
      " [ 0.9434719 ]\n",
      " [ 0.2517122 ]\n",
      " [ 0.8525803 ]\n",
      " [ 0.64795923]\n",
      " [-1.0397258 ]\n",
      " [-1.6153613 ]]\n",
      "step:  52 , cost:  672598.1 hf:  [[ 3.3661566 ]\n",
      " [ 1.5422662 ]\n",
      " [ 0.9467529 ]\n",
      " [ 0.25428736]\n",
      " [ 0.8555962 ]\n",
      " [ 0.6508807 ]\n",
      " [-1.0376849 ]\n",
      " [-1.6133336 ]]\n",
      "step:  53 , cost:  672593.3 hf:  [[ 3.3701088 ]\n",
      " [ 1.5461724 ]\n",
      " [ 0.95003366]\n",
      " [ 0.2568624 ]\n",
      " [ 0.85861206]\n",
      " [ 0.65380216]\n",
      " [-1.0356439 ]\n",
      " [-1.6113057 ]]\n",
      "step:  54 , cost:  672588.44 hf:  [[ 3.374061  ]\n",
      " [ 1.5500787 ]\n",
      " [ 0.95331466]\n",
      " [ 0.25943756]\n",
      " [ 0.86162794]\n",
      " [ 0.6567236 ]\n",
      " [-1.0336031 ]\n",
      " [-1.609278  ]]\n",
      "step:  55 , cost:  672583.56 hf:  [[ 3.378014  ]\n",
      " [ 1.5539851 ]\n",
      " [ 0.9565954 ]\n",
      " [ 0.26201272]\n",
      " [ 0.86464405]\n",
      " [ 0.6596451 ]\n",
      " [-1.0315621 ]\n",
      " [-1.6072502 ]]\n",
      "step:  56 , cost:  672578.7 hf:  [[ 3.3819666 ]\n",
      " [ 1.5578912 ]\n",
      " [ 0.9598764 ]\n",
      " [ 0.26458788]\n",
      " [ 0.8676599 ]\n",
      " [ 0.66256654]\n",
      " [-1.0295212 ]\n",
      " [-1.6052225 ]]\n",
      "step:  57 , cost:  672573.8 hf:  [[ 3.3859189 ]\n",
      " [ 1.5617979 ]\n",
      " [ 0.9631574 ]\n",
      " [ 0.26716292]\n",
      " [ 0.8706758 ]\n",
      " [ 0.665488  ]\n",
      " [-1.0274804 ]\n",
      " [-1.6031947 ]]\n",
      "step:  58 , cost:  672568.94 hf:  [[ 3.3898711 ]\n",
      " [ 1.565704  ]\n",
      " [ 0.9664382 ]\n",
      " [ 0.26973832]\n",
      " [ 0.8736917 ]\n",
      " [ 0.66840947]\n",
      " [-1.0254395 ]\n",
      " [-1.6011668 ]]\n",
      "step:  59 , cost:  672564.0 hf:  [[ 3.3938236 ]\n",
      " [ 1.5696101 ]\n",
      " [ 0.9697192 ]\n",
      " [ 0.27231324]\n",
      " [ 0.87670755]\n",
      " [ 0.6713307 ]\n",
      " [-1.0233986 ]\n",
      " [-1.5991391 ]]\n",
      "step:  60 , cost:  672559.2 hf:  [[ 3.3977766 ]\n",
      " [ 1.5735162 ]\n",
      " [ 0.97299993]\n",
      " [ 0.27488852]\n",
      " [ 0.8797234 ]\n",
      " [ 0.67425215]\n",
      " [-1.0213577 ]\n",
      " [-1.5971113 ]]\n",
      "step:  61 , cost:  672554.3 hf:  [[ 3.4017289 ]\n",
      " [ 1.5774226 ]\n",
      " [ 0.9762809 ]\n",
      " [ 0.27746344]\n",
      " [ 0.8827393 ]\n",
      " [ 0.6771736 ]\n",
      " [-1.0193168 ]\n",
      " [-1.5950836 ]]\n",
      "step:  62 , cost:  672549.44 hf:  [[ 3.4056811 ]\n",
      " [ 1.581329  ]\n",
      " [ 0.9795617 ]\n",
      " [ 0.2800387 ]\n",
      " [ 0.8857552 ]\n",
      " [ 0.68009484]\n",
      " [-1.0172758 ]\n",
      " [-1.5930558 ]]\n",
      "step:  63 , cost:  672544.6 hf:  [[ 3.409634  ]\n",
      " [ 1.5852351 ]\n",
      " [ 0.98284245]\n",
      " [ 0.28261387]\n",
      " [ 0.88877106]\n",
      " [ 0.6830163 ]\n",
      " [-1.015235  ]\n",
      " [-1.591028  ]]\n",
      "step:  64 , cost:  672539.75 hf:  [[ 3.4135864 ]\n",
      " [ 1.5891411 ]\n",
      " [ 0.98612356]\n",
      " [ 0.2851889 ]\n",
      " [ 0.8917868 ]\n",
      " [ 0.68593764]\n",
      " [-1.0131942 ]\n",
      " [-1.5890003 ]]\n",
      "step:  65 , cost:  672534.9 hf:  [[ 3.4175386 ]\n",
      " [ 1.5930476 ]\n",
      " [ 0.9894042 ]\n",
      " [ 0.28776395]\n",
      " [ 0.8948026 ]\n",
      " [ 0.688859  ]\n",
      " [-1.0111535 ]\n",
      " [-1.5869727 ]]\n",
      "step:  66 , cost:  672530.0 hf:  [[ 3.421491  ]\n",
      " [ 1.5969532 ]\n",
      " [ 0.9926851 ]\n",
      " [ 0.290339  ]\n",
      " [ 0.89781857]\n",
      " [ 0.6917803 ]\n",
      " [-1.0091126 ]\n",
      " [-1.5849451 ]]\n",
      "step:  67 , cost:  672525.1 hf:  [[ 3.4254432 ]\n",
      " [ 1.6008596 ]\n",
      " [ 0.99596596]\n",
      " [ 0.2929139 ]\n",
      " [ 0.9008341 ]\n",
      " [ 0.6947017 ]\n",
      " [-1.0070719 ]\n",
      " [-1.5829175 ]]\n",
      "step:  68 , cost:  672520.25 hf:  [[ 3.429396  ]\n",
      " [ 1.6047657 ]\n",
      " [ 0.9992466 ]\n",
      " [ 0.29548895]\n",
      " [ 0.9038501 ]\n",
      " [ 0.697623  ]\n",
      " [-1.0050311 ]\n",
      " [-1.5808898 ]]\n",
      "step:  69 , cost:  672515.4 hf:  [[ 3.4333482 ]\n",
      " [ 1.6086721 ]\n",
      " [ 1.0025272 ]\n",
      " [ 0.298064  ]\n",
      " [ 0.9068656 ]\n",
      " [ 0.70054436]\n",
      " [-1.0029902 ]\n",
      " [-1.5788622 ]]\n",
      "step:  70 , cost:  672510.5 hf:  [[ 3.4373004 ]\n",
      " [ 1.6125782 ]\n",
      " [ 1.0058081 ]\n",
      " [ 0.3006389 ]\n",
      " [ 0.9098816 ]\n",
      " [ 0.70346546]\n",
      " [-1.0009495 ]\n",
      " [-1.5768346 ]]\n",
      "step:  71 , cost:  672505.6 hf:  [[ 3.4412532 ]\n",
      " [ 1.6164842 ]\n",
      " [ 1.0090888 ]\n",
      " [ 0.30321395]\n",
      " [ 0.9128971 ]\n",
      " [ 0.7063868 ]\n",
      " [-0.99890876]\n",
      " [-1.5748069 ]]\n",
      "step:  72 , cost:  672500.75 hf:  [[ 3.4452055 ]\n",
      " [ 1.6203902 ]\n",
      " [ 1.0123696 ]\n",
      " [ 0.305789  ]\n",
      " [ 0.91591287]\n",
      " [ 0.70930815]\n",
      " [-0.996868  ]\n",
      " [-1.5727793 ]]\n",
      "step:  73 , cost:  672495.9 hf:  [[ 3.4491577 ]\n",
      " [ 1.6242962 ]\n",
      " [ 1.0156503 ]\n",
      " [ 0.30836415]\n",
      " [ 0.9189286 ]\n",
      " [ 0.71222925]\n",
      " [-0.99482715]\n",
      " [-1.5707517 ]]\n",
      "step:  74 , cost:  672491.0 hf:  [[ 3.45311   ]\n",
      " [ 1.6282022 ]\n",
      " [ 1.0189312 ]\n",
      " [ 0.31093907]\n",
      " [ 0.9219446 ]\n",
      " [ 0.7151506 ]\n",
      " [-0.9927864 ]\n",
      " [-1.568724  ]]\n",
      "step:  75 , cost:  672486.1 hf:  [[ 3.4570622 ]\n",
      " [ 1.6321082 ]\n",
      " [ 1.022212  ]\n",
      " [ 0.313514  ]\n",
      " [ 0.9249604 ]\n",
      " [ 0.71807194]\n",
      " [-0.99074566]\n",
      " [-1.5666964 ]]\n",
      "step:  76 , cost:  672481.25 hf:  [[ 3.461015  ]\n",
      " [ 1.6360142 ]\n",
      " [ 1.0254927 ]\n",
      " [ 0.31608915]\n",
      " [ 0.92797613]\n",
      " [ 0.7209933 ]\n",
      " [-0.9887049 ]\n",
      " [-1.5646688 ]]\n",
      "step:  77 , cost:  672476.4 hf:  [[ 3.4649673]\n",
      " [ 1.6399207]\n",
      " [ 1.0287733]\n",
      " [ 0.3186642]\n",
      " [ 0.9309919]\n",
      " [ 0.7239146]\n",
      " [-0.9866641]\n",
      " [-1.562641 ]]\n",
      "step:  78 , cost:  672471.5 hf:  [[ 3.4689195 ]\n",
      " [ 1.6438267 ]\n",
      " [ 1.032054  ]\n",
      " [ 0.3212391 ]\n",
      " [ 0.93400764]\n",
      " [ 0.72683597]\n",
      " [-0.9846234 ]\n",
      " [-1.5606134 ]]\n",
      "step:  79 , cost:  672466.6 hf:  [[ 3.4728718 ]\n",
      " [ 1.6477327 ]\n",
      " [ 1.0353348 ]\n",
      " [ 0.32381415]\n",
      " [ 0.9370234 ]\n",
      " [ 0.7297571 ]\n",
      " [-0.9825826 ]\n",
      " [-1.5585859 ]]\n",
      "step:  80 , cost:  672461.8 hf:  [[ 3.476824  ]\n",
      " [ 1.6516387 ]\n",
      " [ 1.0386155 ]\n",
      " [ 0.32638907]\n",
      " [ 0.9400389 ]\n",
      " [ 0.7326782 ]\n",
      " [-0.9805418 ]\n",
      " [-1.5565583 ]]\n",
      "step:  81 , cost:  672457.0 hf:  [[ 3.4807763 ]\n",
      " [ 1.6555448 ]\n",
      " [ 1.0418961 ]\n",
      " [ 0.3289641 ]\n",
      " [ 0.9430547 ]\n",
      " [ 0.7355995 ]\n",
      " [-0.97850114]\n",
      " [-1.5545306 ]]\n",
      "step:  82 , cost:  672452.06 hf:  [[ 3.4847286 ]\n",
      " [ 1.6594503 ]\n",
      " [ 1.0451765 ]\n",
      " [ 0.33153892]\n",
      " [ 0.94607043]\n",
      " [ 0.7385206 ]\n",
      " [-0.9764604 ]\n",
      " [-1.552503  ]]\n",
      "step:  83 , cost:  672447.1 hf:  [[ 3.4886804 ]\n",
      " [ 1.6633561 ]\n",
      " [ 1.0484571 ]\n",
      " [ 0.33411372]\n",
      " [ 0.9490857 ]\n",
      " [ 0.7414417 ]\n",
      " [-0.97441965]\n",
      " [-1.5504754 ]]\n",
      "step:  84 , cost:  672442.3 hf:  [[ 3.4926322 ]\n",
      " [ 1.6672618 ]\n",
      " [ 1.0517378 ]\n",
      " [ 0.33668876]\n",
      " [ 0.95210147]\n",
      " [ 0.7443626 ]\n",
      " [-0.97237897]\n",
      " [-1.5484477 ]]\n",
      "step:  85 , cost:  672437.5 hf:  [[ 3.4965844 ]\n",
      " [ 1.6711677 ]\n",
      " [ 1.0550181 ]\n",
      " [ 0.33926356]\n",
      " [ 0.95511687]\n",
      " [ 0.7472838 ]\n",
      " [-0.97033834]\n",
      " [-1.5464202 ]]\n",
      "step:  86 , cost:  672432.6 hf:  [[ 3.5005364 ]\n",
      " [ 1.6750734 ]\n",
      " [ 1.0582986 ]\n",
      " [ 0.34183812]\n",
      " [ 0.95813227]\n",
      " [ 0.7502048 ]\n",
      " [-0.9682977 ]\n",
      " [-1.5443927 ]]\n",
      "step:  87 , cost:  672427.75 hf:  [[ 3.504488  ]\n",
      " [ 1.678979  ]\n",
      " [ 1.0615789 ]\n",
      " [ 0.34441304]\n",
      " [ 0.9611479 ]\n",
      " [ 0.7531258 ]\n",
      " [-0.9662571 ]\n",
      " [-1.5423653 ]]\n",
      "step:  88 , cost:  672422.9 hf:  [[ 3.5084398 ]\n",
      " [ 1.6828849 ]\n",
      " [ 1.0648596 ]\n",
      " [ 0.34698784]\n",
      " [ 0.9641633 ]\n",
      " [ 0.756047  ]\n",
      " [-0.96421653]\n",
      " [-1.5403378 ]]\n",
      "step:  89 , cost:  672418.0 hf:  [[ 3.512392  ]\n",
      " [ 1.6867908 ]\n",
      " [ 1.0681399 ]\n",
      " [ 0.34956253]\n",
      " [ 0.9671787 ]\n",
      " [ 0.75896776]\n",
      " [-0.9621759 ]\n",
      " [-1.5383103 ]]\n",
      "step:  90 , cost:  672413.1 hf:  [[ 3.516344  ]\n",
      " [ 1.6906962 ]\n",
      " [ 1.0714202 ]\n",
      " [ 0.35213733]\n",
      " [ 0.97019434]\n",
      " [ 0.76188874]\n",
      " [-0.9601353 ]\n",
      " [-1.5362828 ]]\n",
      "step:  91 , cost:  672408.25 hf:  [[ 3.5202956]\n",
      " [ 1.6946017]\n",
      " [ 1.074701 ]\n",
      " [ 0.3547119]\n",
      " [ 0.9732095]\n",
      " [ 0.7648097]\n",
      " [-0.9580947]\n",
      " [-1.5342553]]\n",
      "step:  92 , cost:  672403.4 hf:  [[ 3.5242474 ]\n",
      " [ 1.6985075 ]\n",
      " [ 1.0779812 ]\n",
      " [ 0.3572868 ]\n",
      " [ 0.97622514]\n",
      " [ 0.7677307 ]\n",
      " [-0.9560541 ]\n",
      " [-1.5322279 ]]\n",
      "step:  93 , cost:  672398.5 hf:  [[ 3.5281997 ]\n",
      " [ 1.702413  ]\n",
      " [ 1.0812615 ]\n",
      " [ 0.3598616 ]\n",
      " [ 0.97924054]\n",
      " [ 0.7706517 ]\n",
      " [-0.95401347]\n",
      " [-1.5302004 ]]\n",
      "step:  94 , cost:  672393.6 hf:  [[ 3.5321517 ]\n",
      " [ 1.7063189 ]\n",
      " [ 1.0845418 ]\n",
      " [ 0.3624363 ]\n",
      " [ 0.98225594]\n",
      " [ 0.7735727 ]\n",
      " [-0.9519729 ]\n",
      " [-1.5281729 ]]\n",
      "step:  95 , cost:  672388.75 hf:  [[ 3.5361032 ]\n",
      " [ 1.7102245 ]\n",
      " [ 1.0878223 ]\n",
      " [ 0.3650111 ]\n",
      " [ 0.98527133]\n",
      " [ 0.77649367]\n",
      " [-0.9499323 ]\n",
      " [-1.5261455 ]]\n",
      "step:  96 , cost:  672383.9 hf:  [[ 3.540055  ]\n",
      " [ 1.7141302 ]\n",
      " [ 1.0911028 ]\n",
      " [ 0.3675859 ]\n",
      " [ 0.98828673]\n",
      " [ 0.77941465]\n",
      " [-0.94789165]\n",
      " [-1.524118  ]]\n",
      "step:  97 , cost:  672379.0 hf:  [[ 3.5440073 ]\n",
      " [ 1.7180358 ]\n",
      " [ 1.0943831 ]\n",
      " [ 0.37016058]\n",
      " [ 0.99130213]\n",
      " [ 0.78233564]\n",
      " [-0.9458511 ]\n",
      " [-1.5220904 ]]\n",
      "step:  98 , cost:  672374.1 hf:  [[ 3.5479593 ]\n",
      " [ 1.7219415 ]\n",
      " [ 1.0976636 ]\n",
      " [ 0.3727355 ]\n",
      " [ 0.994318  ]\n",
      " [ 0.78525686]\n",
      " [-0.94381046]\n",
      " [-1.5200629 ]]\n",
      "step:  99 , cost:  672369.25 hf:  [[ 3.5519109 ]\n",
      " [ 1.7258474 ]\n",
      " [ 1.1009442 ]\n",
      " [ 0.3753103 ]\n",
      " [ 0.9973334 ]\n",
      " [ 0.78817785]\n",
      " [-0.94176984]\n",
      " [-1.5180355 ]]\n",
      "step:  100 , cost:  672364.4 hf:  [[ 3.5558627 ]\n",
      " [ 1.729753  ]\n",
      " [ 1.1042244 ]\n",
      " [ 0.37788486]\n",
      " [ 1.0003488 ]\n",
      " [ 0.7910986 ]\n",
      " [-0.9397293 ]\n",
      " [-1.516008  ]]\n"
     ]
    }
   ],
   "source": [
    "for step in range(101):\n",
    "    cv, hv, _ = sess.run([cost, hf, train], feed_dict={x:xdata, y:ydata})\n",
    "    print(\"step: \", step, \", cost: \", cv, \"hf: \", hv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax 사용 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [[1, 2, 1, 1],\n",
    "          [2, 1, 3, 2],\n",
    "          [3, 1, 3, 4],\n",
    "          [4, 1, 5, 5],\n",
    "          [1, 7, 5, 5],\n",
    "          [1, 2, 5, 6],\n",
    "          [1, 6, 6, 6],\n",
    "          [1, 7, 7, 7]]\n",
    "#y값은 one hot 인코딩 방식으로 초기화 함...(a,b,c)중 하나를 hot하게 함...\n",
    "y_data = [[0, 0, 1],#2\n",
    "          [0, 0, 1],#2\n",
    "          [0, 0, 1],#2\n",
    "          [0, 1, 0],#1\n",
    "          [0, 1, 0],#1\n",
    "          [0, 1, 0],#1\n",
    "          [1, 0, 0],#0\n",
    "          [1, 0, 0]]#0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 3\n",
    "\n",
    "x = tf.placeholder(\"float\", [None, 4])\n",
    "y = tf.placeholder(\"float\", [None, num_classes])\n",
    "w = tf.Variable(tf.random_normal([4, num_classes]))\n",
    "b = tf.Variable(tf.random_normal([num_classes]))\n",
    "\n",
    "hf = tf.nn.softmax(tf.matmul(x, w) + b)\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(y * tf.log(hf), axis=1))\n",
    "train = tf.train.GradientDescentOptimizer(0.1).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.3370667\n",
      "200 0.5028758\n",
      "400 0.4246089\n",
      "600 0.35452634\n",
      "800 0.28400964\n",
      "1000 0.23019987\n",
      "1200 0.20906056\n",
      "1400 0.19132608\n",
      "1600 0.17624602\n",
      "1800 0.16328058\n",
      "2000 0.1520252\n",
      "==================================================\n",
      "[1]\n",
      "==================================================\n",
      "[1 0 2]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(2001):\n",
    "        sess.run(train, feed_dict={x:x_data, y:y_data})\n",
    "        if step % 200 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={x:x_data, y:y_data}))\n",
    "    print(\"=\" * 50)\n",
    "    # 만약 x_data가 1, 11, 7, 9라면 y는?\n",
    "    res = sess.run(hf, feed_dict={x:[[1, 11, 7, 9]]})\n",
    "    print(sess.run(tf.argmax(res, axis=1)))    # tf.argmax(): 최대값에 해당하는 인수의 index를 리턴하는 함수\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    res2 = sess.run(hf, feed_dict={x:[[1, 11, 7, 9], [1, 3, 4, 3], [1, 1, 0, 1]]})\n",
    "    print(sess.run(tf.argmax(res2, axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## softmax_cross_entropy_with_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예제 2: softmax, softmax_cross_entropy_with_logits, 원핫인코딩 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101, 17)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xy = np.loadtxt(\"data/zoo.csv\", delimiter=\",\", dtype=np.float32)\n",
    "xy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101, 1)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xdata = xy[:, :-1]\n",
    "ydata = xy[:, [-1]]\n",
    "xdata.shape\n",
    "ydata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one_hot Tensor(\"one_hot_2:0\", shape=(?, 1, 7), dtype=float32)\n",
      "one_hot after reshape Tensor(\"Reshape_1:0\", shape=(?, 7), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 16])\n",
    "y = tf.placeholder(tf.int32, [None, 1])\n",
    "y_one_hot = tf.one_hot(y, 7)    # tf.one_hot(원핫인코딩이 안된 자료, 길이): 원핫인코딩 해주는 함수\n",
    "# one hot 함수는 한 차원 높게 변환된다\n",
    "# y: [[0], [3], ..., [5]] => one_hot => [[[1000000]], [[0001000]], ..., [[0000010]]]\n",
    "# 위의 예시에서 shape이 [None, 1] 에서 [None, 1, 7] 로 바뀐다\n",
    "# 우리가 바라는 출력결과의 shape은 [None, 7] 이다\n",
    "# 그러므로 reshape을 하면 된다. tf.reshape(무엇을, 무엇으로)\n",
    "# 예시: tf.shape(y_one_hot, [-1, 7])\n",
    "# 예시 결과: [[[1000000]], [[0001000]], ..., [[0000010]]] => [[1000000]], [[0001000]], ..., [[0000010]]\n",
    "print(\"one_hot\", y_one_hot)\n",
    "y_one_hot = tf.reshape(y_one_hot, [-1, 7])\n",
    "print(\"one_hot after reshape\", y_one_hot)\n",
    "\n",
    "# 0~6: 7가지 종류의 동물 -> 분류기 7개\n",
    "# w1x1 + w2x2 + ... + w7x7 + b => softmax => 0번 종류의 동물에 대한 확률\n",
    "# w1x1 + w2x2 + ... + w7x7 + b => softmax => 1번 종류의 동물에 대한 확률\n",
    "# ...\n",
    "# w1x1 + w2x2 + ... + w7x7 + b => softmax => 6번 종류의 동물에 대한 확률\n",
    "\n",
    "# 1000000, 0100000, ... => [None, 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = tf.Variable(tf.random_normal([16, 7]))\n",
    "# x:[None,16] 그러므로 w:[16,7]\n",
    "b = tf.Variable(tf.random_normal([7]))\n",
    "\n",
    "logit = tf.matmul(x, w) + b\n",
    "hf = tf.nn.softmax(logit)\n",
    "cost = tf.nn.softmax_cross_entropy_with_logits(logits=logit, labels=y_one_hot)\n",
    "cost2 = tf.reduce_mean(cost)\n",
    "train = tf.train.GradientDescentOptimizer(0.1).minimize(cost2)\n",
    "\n",
    "prediction = tf.argmax(hf, axis=1)\n",
    "correct_prediction = tf.equal(prediction, tf.argmax(y_one_hot, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, cost: 6.5676494, accuracy: 0.00990099\n",
      "step: 100, cost: 0.6870523, accuracy: 0.76237625\n",
      "step: 200, cost: 0.414034, accuracy: 0.8811881\n",
      "step: 300, cost: 0.29852012, accuracy: 0.9207921\n",
      "step: 400, cost: 0.23260202, accuracy: 0.96039605\n",
      "step: 500, cost: 0.18965861, accuracy: 0.97029704\n",
      "step: 600, cost: 0.15950668, accuracy: 0.980198\n",
      "step: 700, cost: 0.13728254, accuracy: 1.0\n",
      "step: 800, cost: 0.12033349, accuracy: 1.0\n",
      "step: 900, cost: 0.107068375, accuracy: 1.0\n",
      "step: 1000, cost: 0.096462965, accuracy: 1.0\n",
      "step: 1100, cost: 0.08782466, accuracy: 1.0\n",
      "step: 1200, cost: 0.08066996, accuracy: 1.0\n",
      "step: 1300, cost: 0.07465393, accuracy: 1.0\n",
      "step: 1400, cost: 0.0695262, accuracy: 1.0\n",
      "step: 1500, cost: 0.06510257, accuracy: 1.0\n",
      "step: 1600, cost: 0.06124528, accuracy: 1.0\n",
      "step: 1700, cost: 0.057849813, accuracy: 1.0\n",
      "step: 1800, cost: 0.054835808, accuracy: 1.0\n",
      "step: 1900, cost: 0.05214057, accuracy: 1.0\n",
      "step: 2000, cost: 0.04971455, accuracy: 1.0\n",
      "step: 2100, cost: 0.04751793, accuracy: 1.0\n",
      "step: 2200, cost: 0.04551875, accuracy: 1.0\n",
      "step: 2300, cost: 0.043690596, accuracy: 1.0\n",
      "step: 2400, cost: 0.042011775, accuracy: 1.0\n",
      "step: 2500, cost: 0.040464133, accuracy: 1.0\n",
      "step: 2600, cost: 0.039032415, accuracy: 1.0\n",
      "step: 2700, cost: 0.037703685, accuracy: 1.0\n",
      "step: 2800, cost: 0.03646691, accuracy: 1.0\n",
      "step: 2900, cost: 0.035312578, accuracy: 1.0\n",
      "step: 3000, cost: 0.034232523, accuracy: 1.0\n",
      "step: 3100, cost: 0.033219617, accuracy: 1.0\n",
      "step: 3200, cost: 0.032267626, accuracy: 1.0\n",
      "step: 3300, cost: 0.03137105, accuracy: 1.0\n",
      "step: 3400, cost: 0.030525135, accuracy: 1.0\n",
      "step: 3500, cost: 0.029725585, accuracy: 1.0\n",
      "step: 3600, cost: 0.028968545, accuracy: 1.0\n",
      "step: 3700, cost: 0.028250694, accuracy: 1.0\n",
      "step: 3800, cost: 0.027569003, accuracy: 1.0\n",
      "step: 3900, cost: 0.026920749, accuracy: 1.0\n",
      "step: 4000, cost: 0.026303457, accuracy: 1.0\n",
      "step: 4100, cost: 0.02571492, accuracy: 1.0\n",
      "step: 4200, cost: 0.025153188, accuracy: 1.0\n",
      "step: 4300, cost: 0.024616348, accuracy: 1.0\n",
      "step: 4400, cost: 0.024102788, accuracy: 1.0\n",
      "step: 4500, cost: 0.023611035, accuracy: 1.0\n",
      "step: 4600, cost: 0.02313966, accuracy: 1.0\n",
      "step: 4700, cost: 0.022687417, accuracy: 1.0\n",
      "step: 4800, cost: 0.022253105, accuracy: 1.0\n",
      "step: 4900, cost: 0.021835703, accuracy: 1.0\n",
      "step: 5000, cost: 0.02143422, accuracy: 1.0\n",
      "step: 5100, cost: 0.021047732, accuracy: 1.0\n",
      "step: 5200, cost: 0.02067541, accuracy: 1.0\n",
      "step: 5300, cost: 0.020316457, accuracy: 1.0\n",
      "step: 5400, cost: 0.019970221, accuracy: 1.0\n",
      "step: 5500, cost: 0.019635912, accuracy: 1.0\n",
      "step: 5600, cost: 0.019313026, accuracy: 1.0\n",
      "step: 5700, cost: 0.019000888, accuracy: 1.0\n",
      "step: 5800, cost: 0.018699039, accuracy: 1.0\n",
      "step: 5900, cost: 0.01840691, accuracy: 1.0\n",
      "step: 6000, cost: 0.018124063, accuracy: 1.0\n",
      "step: 6100, cost: 0.01785002, accuracy: 1.0\n",
      "step: 6200, cost: 0.01758445, accuracy: 1.0\n",
      "step: 6300, cost: 0.017326908, accuracy: 1.0\n",
      "step: 6400, cost: 0.017077003, accuracy: 1.0\n",
      "step: 6500, cost: 0.016834432, accuracy: 1.0\n",
      "step: 6600, cost: 0.01659883, accuracy: 1.0\n",
      "step: 6700, cost: 0.016369969, accuracy: 1.0\n",
      "step: 6800, cost: 0.016147496, accuracy: 1.0\n",
      "step: 6900, cost: 0.015931193, accuracy: 1.0\n",
      "step: 7000, cost: 0.01572076, accuracy: 1.0\n",
      "step: 7100, cost: 0.0155159775, accuracy: 1.0\n",
      "step: 7200, cost: 0.015316633, accuracy: 1.0\n",
      "step: 7300, cost: 0.015122457, accuracy: 1.0\n",
      "step: 7400, cost: 0.014933279, accuracy: 1.0\n",
      "step: 7500, cost: 0.014748939, accuracy: 1.0\n",
      "step: 7600, cost: 0.014569221, accuracy: 1.0\n",
      "step: 7700, cost: 0.014393927, accuracy: 1.0\n",
      "step: 7800, cost: 0.014222972, accuracy: 1.0\n",
      "step: 7900, cost: 0.014056083, accuracy: 1.0\n",
      "step: 8000, cost: 0.013893189, accuracy: 1.0\n",
      "step: 8100, cost: 0.013734134, accuracy: 1.0\n",
      "step: 8200, cost: 0.013578773, accuracy: 1.0\n",
      "step: 8300, cost: 0.013427002, accuracy: 1.0\n",
      "step: 8400, cost: 0.013278657, accuracy: 1.0\n",
      "step: 8500, cost: 0.013133648, accuracy: 1.0\n",
      "step: 8600, cost: 0.012991849, accuracy: 1.0\n",
      "step: 8700, cost: 0.012853163, accuracy: 1.0\n",
      "step: 8800, cost: 0.012717491, accuracy: 1.0\n",
      "step: 8900, cost: 0.012584726, accuracy: 1.0\n",
      "step: 9000, cost: 0.012454787, accuracy: 1.0\n",
      "step: 9100, cost: 0.012327525, accuracy: 1.0\n",
      "step: 9200, cost: 0.012202924, accuracy: 1.0\n",
      "step: 9300, cost: 0.012080907, accuracy: 1.0\n",
      "step: 9400, cost: 0.0119613595, accuracy: 1.0\n",
      "step: 9500, cost: 0.011844214, accuracy: 1.0\n",
      "step: 9600, cost: 0.011729362, accuracy: 1.0\n",
      "step: 9700, cost: 0.011616808, accuracy: 1.0\n",
      "step: 9800, cost: 0.011506463, accuracy: 1.0\n",
      "step: 9900, cost: 0.011398228, accuracy: 1.0\n",
      "step: 10000, cost: 0.011292056, accuracy: 1.0\n",
      "==================================================\n",
      "[0 0 3 0 0 0 0 3 3 0 0 1 3 6 6 6 1 0 3 0 1 1 0 1 5 4 4 0 0 0 5 0 0 1 3 0 0\n",
      " 1 3 5 5 1 5 1 0 0 6 0 0 0 0 5 4 6 0 0 1 1 1 1 3 3 2 0 0 0 0 0 0 0 0 1 6 3\n",
      " 0 0 2 6 1 1 2 6 3 1 0 6 3 1 5 4 2 2 3 0 0 1 0 5 0 6 1]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict={x:xdata, y:ydata})\n",
    "        if step % 100 == 0:\n",
    "            cv, av = sess.run([cost2, accuracy], feed_dict={x:xdata, y:ydata})\n",
    "            print(\"step: %s, cost: %s, accuracy: %s\" % (step, cv, av))\n",
    "# 전체 데이터로 트레이닝 수행하여 모델 생성\n",
    "# 모델에 전체 데이터를 집어넣어서 정확도를 출력\n",
    "# 데이터를 분할하지 않았음\n",
    "    print(\"=\" * 50)\n",
    "    pred = sess.run(prediction, feed_dict={x:xdata})\n",
    "    print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## flatten 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "flatten 함수: 차원을 한 차원 내려주는 함수\n",
    "\n",
    "예: [[1], [0]] => [1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, cost: 5.213575, accuracy: 0.22772278\n",
      "step: 100, cost: 0.8572766, accuracy: 0.6930693\n",
      "step: 200, cost: 0.48454756, accuracy: 0.8613861\n",
      "step: 300, cost: 0.35002664, accuracy: 0.9009901\n",
      "step: 400, cost: 0.27447465, accuracy: 0.9306931\n",
      "step: 500, cost: 0.2252, accuracy: 0.9306931\n",
      "step: 600, cost: 0.19054726, accuracy: 0.95049506\n",
      "step: 700, cost: 0.16482995, accuracy: 0.96039605\n",
      "step: 800, cost: 0.14496155, accuracy: 0.96039605\n",
      "step: 900, cost: 0.12914348, accuracy: 0.97029704\n",
      "step: 1000, cost: 0.11625838, accuracy: 0.980198\n",
      "step: 1100, cost: 0.10557321, accuracy: 0.990099\n",
      "step: 1200, cost: 0.096584134, accuracy: 0.990099\n",
      "step: 1300, cost: 0.088931575, accuracy: 0.990099\n",
      "step: 1400, cost: 0.082350336, accuracy: 1.0\n",
      "step: 1500, cost: 0.07664005, accuracy: 1.0\n",
      "step: 1600, cost: 0.0716462, accuracy: 1.0\n",
      "step: 1700, cost: 0.067247696, accuracy: 1.0\n",
      "step: 1800, cost: 0.063348405, accuracy: 1.0\n",
      "step: 1900, cost: 0.05987115, accuracy: 1.0\n",
      "step: 2000, cost: 0.05675336, accuracy: 1.0\n",
      "step: 2100, cost: 0.05394381, accuracy: 1.0\n",
      "step: 2200, cost: 0.05140027, accuracy: 1.0\n",
      "step: 2300, cost: 0.049087703, accuracy: 1.0\n",
      "step: 2400, cost: 0.046976693, accuracy: 1.0\n",
      "step: 2500, cost: 0.045042604, accuracy: 1.0\n",
      "step: 2600, cost: 0.043264408, accuracy: 1.0\n",
      "step: 2700, cost: 0.04162435, accuracy: 1.0\n",
      "step: 2800, cost: 0.04010706, accuracy: 1.0\n",
      "step: 2900, cost: 0.03869948, accuracy: 1.0\n",
      "step: 3000, cost: 0.03739019, accuracy: 1.0\n",
      "step: 3100, cost: 0.036169298, accuracy: 1.0\n",
      "step: 3200, cost: 0.03502821, accuracy: 1.0\n",
      "step: 3300, cost: 0.03395933, accuracy: 1.0\n",
      "step: 3400, cost: 0.032956038, accuracy: 1.0\n",
      "step: 3500, cost: 0.032012515, accuracy: 1.0\n",
      "step: 3600, cost: 0.031123478, accuracy: 1.0\n",
      "step: 3700, cost: 0.030284403, accuracy: 1.0\n",
      "step: 3800, cost: 0.029491117, accuracy: 1.0\n",
      "step: 3900, cost: 0.028739983, accuracy: 1.0\n",
      "step: 4000, cost: 0.028027652, accuracy: 1.0\n",
      "step: 4100, cost: 0.027351245, accuracy: 1.0\n",
      "step: 4200, cost: 0.026708065, accuracy: 1.0\n",
      "step: 4300, cost: 0.026095627, accuracy: 1.0\n",
      "step: 4400, cost: 0.025511833, accuracy: 1.0\n",
      "step: 4500, cost: 0.02495467, accuracy: 1.0\n",
      "step: 4600, cost: 0.024422308, accuracy: 1.0\n",
      "step: 4700, cost: 0.023913173, accuracy: 1.0\n",
      "step: 4800, cost: 0.023425642, accuracy: 1.0\n",
      "step: 4900, cost: 0.0229585, accuracy: 1.0\n",
      "step: 5000, cost: 0.02251035, accuracy: 1.0\n",
      "step: 5100, cost: 0.022080129, accuracy: 1.0\n",
      "step: 5200, cost: 0.021666687, accuracy: 1.0\n",
      "step: 5300, cost: 0.021269057, accuracy: 1.0\n",
      "step: 5400, cost: 0.020886356, accuracy: 1.0\n",
      "step: 5500, cost: 0.020517807, accuracy: 1.0\n",
      "step: 5600, cost: 0.020162497, accuracy: 1.0\n",
      "step: 5700, cost: 0.019819835, accuracy: 1.0\n",
      "step: 5800, cost: 0.01948903, accuracy: 1.0\n",
      "step: 5900, cost: 0.019169588, accuracy: 1.0\n",
      "step: 6000, cost: 0.018860826, accuracy: 1.0\n",
      "step: 6100, cost: 0.018562239, accuracy: 1.0\n",
      "step: 6200, cost: 0.01827336, accuracy: 1.0\n",
      "step: 6300, cost: 0.017993674, accuracy: 1.0\n",
      "step: 6400, cost: 0.017722744, accuracy: 1.0\n",
      "step: 6500, cost: 0.017460156, accuracy: 1.0\n",
      "step: 6600, cost: 0.017205505, accuracy: 1.0\n",
      "step: 6700, cost: 0.016958516, accuracy: 1.0\n",
      "step: 6800, cost: 0.016718764, accuracy: 1.0\n",
      "step: 6900, cost: 0.016485931, accuracy: 1.0\n",
      "step: 7000, cost: 0.016259734, accuracy: 1.0\n",
      "step: 7100, cost: 0.016039899, accuracy: 1.0\n",
      "step: 7200, cost: 0.015826114, accuracy: 1.0\n",
      "step: 7300, cost: 0.015618184, accuracy: 1.0\n",
      "step: 7400, cost: 0.0154158315, accuracy: 1.0\n",
      "step: 7500, cost: 0.015218884, accuracy: 1.0\n",
      "step: 7600, cost: 0.015027014, accuracy: 1.0\n",
      "step: 7700, cost: 0.014840119, accuracy: 1.0\n",
      "step: 7800, cost: 0.014657964, accuracy: 1.0\n",
      "step: 7900, cost: 0.014480398, accuracy: 1.0\n",
      "step: 8000, cost: 0.014307239, accuracy: 1.0\n",
      "step: 8100, cost: 0.0141382925, accuracy: 1.0\n",
      "step: 8200, cost: 0.013973429, accuracy: 1.0\n",
      "step: 8300, cost: 0.013812507, accuracy: 1.0\n",
      "step: 8400, cost: 0.013655363, accuracy: 1.0\n",
      "step: 8500, cost: 0.013501872, accuracy: 1.0\n",
      "step: 8600, cost: 0.013351889, accuracy: 1.0\n",
      "step: 8700, cost: 0.013205299, accuracy: 1.0\n",
      "step: 8800, cost: 0.013062022, accuracy: 1.0\n",
      "step: 8900, cost: 0.012921955, accuracy: 1.0\n",
      "step: 9000, cost: 0.012784902, accuracy: 1.0\n",
      "step: 9100, cost: 0.01265079, accuracy: 1.0\n",
      "step: 9200, cost: 0.012519623, accuracy: 1.0\n",
      "step: 9300, cost: 0.012391206, accuracy: 1.0\n",
      "step: 9400, cost: 0.012265457, accuracy: 1.0\n",
      "step: 9500, cost: 0.01214233, accuracy: 1.0\n",
      "step: 9600, cost: 0.012021724, accuracy: 1.0\n",
      "step: 9700, cost: 0.011903519, accuracy: 1.0\n",
      "step: 9800, cost: 0.011787722, accuracy: 1.0\n",
      "step: 9900, cost: 0.01167421, accuracy: 1.0\n",
      "step: 10000, cost: 0.011562939, accuracy: 1.0\n",
      "==================================================\n",
      "[True] 예측: 0, 실제 y: 0\n",
      "[True] 예측: 0, 실제 y: 0\n",
      "[True] 예측: 3, 실제 y: 3\n",
      "[True] 예측: 0, 실제 y: 0\n",
      "[True] 예측: 0, 실제 y: 0\n",
      "[True] 예측: 0, 실제 y: 0\n",
      "[True] 예측: 0, 실제 y: 0\n",
      "[True] 예측: 3, 실제 y: 3\n",
      "[True] 예측: 3, 실제 y: 3\n",
      "[True] 예측: 0, 실제 y: 0\n",
      "[True] 예측: 0, 실제 y: 0\n",
      "[True] 예측: 1, 실제 y: 1\n",
      "[True] 예측: 3, 실제 y: 3\n",
      "[True] 예측: 6, 실제 y: 6\n",
      "[True] 예측: 6, 실제 y: 6\n",
      "[True] 예측: 6, 실제 y: 6\n",
      "[True] 예측: 1, 실제 y: 1\n",
      "[True] 예측: 0, 실제 y: 0\n",
      "[True] 예측: 3, 실제 y: 3\n",
      "[True] 예측: 0, 실제 y: 0\n",
      "[True] 예측: 1, 실제 y: 1\n",
      "[True] 예측: 1, 실제 y: 1\n",
      "[True] 예측: 0, 실제 y: 0\n",
      "[True] 예측: 1, 실제 y: 1\n",
      "[True] 예측: 5, 실제 y: 5\n",
      "[True] 예측: 4, 실제 y: 4\n",
      "[True] 예측: 4, 실제 y: 4\n",
      "[True] 예측: 0, 실제 y: 0\n",
      "[True] 예측: 0, 실제 y: 0\n",
      "[True] 예측: 0, 실제 y: 0\n",
      "[True] 예측: 5, 실제 y: 5\n",
      "[True] 예측: 0, 실제 y: 0\n",
      "[True] 예측: 0, 실제 y: 0\n",
      "[True] 예측: 1, 실제 y: 1\n",
      "[True] 예측: 3, 실제 y: 3\n",
      "[True] 예측: 0, 실제 y: 0\n",
      "[True] 예측: 0, 실제 y: 0\n",
      "[True] 예측: 1, 실제 y: 1\n",
      "[True] 예측: 3, 실제 y: 3\n",
      "[True] 예측: 5, 실제 y: 5\n",
      "[True] 예측: 5, 실제 y: 5\n",
      "[True] 예측: 1, 실제 y: 1\n",
      "[True] 예측: 5, 실제 y: 5\n",
      "[True] 예측: 1, 실제 y: 1\n",
      "[True] 예측: 0, 실제 y: 0\n",
      "[True] 예측: 0, 실제 y: 0\n",
      "[True] 예측: 6, 실제 y: 6\n",
      "[True] 예측: 0, 실제 y: 0\n",
      "[True] 예측: 0, 실제 y: 0\n",
      "[True] 예측: 0, 실제 y: 0\n",
      "[True] 예측: 0, 실제 y: 0\n",
      "[True] 예측: 5, 실제 y: 5\n",
      "[True] 예측: 4, 실제 y: 4\n",
      "[True] 예측: 6, 실제 y: 6\n",
      "[True] 예측: 0, 실제 y: 0\n",
      "[True] 예측: 0, 실제 y: 0\n",
      "[True] 예측: 1, 실제 y: 1\n",
      "[True] 예측: 1, 실제 y: 1\n",
      "[True] 예측: 1, 실제 y: 1\n",
      "[True] 예측: 1, 실제 y: 1\n",
      "[True] 예측: 3, 실제 y: 3\n",
      "[True] 예측: 3, 실제 y: 3\n",
      "[True] 예측: 2, 실제 y: 2\n",
      "[True] 예측: 0, 실제 y: 0\n",
      "[True] 예측: 0, 실제 y: 0\n",
      "[True] 예측: 0, 실제 y: 0\n",
      "[True] 예측: 0, 실제 y: 0\n",
      "[True] 예측: 0, 실제 y: 0\n",
      "[True] 예측: 0, 실제 y: 0\n",
      "[True] 예측: 0, 실제 y: 0\n",
      "[True] 예측: 0, 실제 y: 0\n",
      "[True] 예측: 1, 실제 y: 1\n",
      "[True] 예측: 6, 실제 y: 6\n",
      "[True] 예측: 3, 실제 y: 3\n",
      "[True] 예측: 0, 실제 y: 0\n",
      "[True] 예측: 0, 실제 y: 0\n",
      "[True] 예측: 2, 실제 y: 2\n",
      "[True] 예측: 6, 실제 y: 6\n",
      "[True] 예측: 1, 실제 y: 1\n",
      "[True] 예측: 1, 실제 y: 1\n",
      "[True] 예측: 2, 실제 y: 2\n",
      "[True] 예측: 6, 실제 y: 6\n",
      "[True] 예측: 3, 실제 y: 3\n",
      "[True] 예측: 1, 실제 y: 1\n",
      "[True] 예측: 0, 실제 y: 0\n",
      "[True] 예측: 6, 실제 y: 6\n",
      "[True] 예측: 3, 실제 y: 3\n",
      "[True] 예측: 1, 실제 y: 1\n",
      "[True] 예측: 5, 실제 y: 5\n",
      "[True] 예측: 4, 실제 y: 4\n",
      "[True] 예측: 2, 실제 y: 2\n",
      "[True] 예측: 2, 실제 y: 2\n",
      "[True] 예측: 3, 실제 y: 3\n",
      "[True] 예측: 0, 실제 y: 0\n",
      "[True] 예측: 0, 실제 y: 0\n",
      "[True] 예측: 1, 실제 y: 1\n",
      "[True] 예측: 0, 실제 y: 0\n",
      "[True] 예측: 5, 실제 y: 5\n",
      "[True] 예측: 0, 실제 y: 0\n",
      "[True] 예측: 6, 실제 y: 6\n",
      "[True] 예측: 1, 실제 y: 1\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict={x:xdata, y:ydata})\n",
    "        if step % 100 == 0:\n",
    "            cv, av = sess.run([cost2, accuracy], feed_dict={x:xdata, y:ydata})\n",
    "            print(\"step: %s, cost: %s, accuracy: %s\" % (step, cv, av))\n",
    "\n",
    "    print(\"=\" * 50)\n",
    "    pred = sess.run(prediction, feed_dict={x:xdata})\n",
    "    for p, y in zip(pred, ydata.flatten()):\n",
    "        print(\"[{}] 예측: {}, 실제 y: {}\".format(p==int(y), p, int(y)))\n",
    "#     print(sess.run(prediction, feed_dict={x:[[...]]}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

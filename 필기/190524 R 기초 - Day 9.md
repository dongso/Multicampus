# Day 9

## 오늘 다룰 내용들

* TF-IDF(Term Frequency - Inverse Document Frequency)
  * 정보 검색과 텍스트 마이닝에서 이용하는 가중치
  * 여러 문서로 이루어진 문서군이 있을 때 어떤 단어가 특정 문서 내에서 얼마나 중요한 것인지를 나타내는 통계적 수치
  * 문장 속에서 중요한 단어가 무엇인지, 그 단어가 긍정인지 부정인지 판단할 때 중요하다

* Bag of Words(BoW)
  * 서로 다른 두 문장이 있을때, 그 두 문장이 어느 정도로 유사한지 그 유사한 정도를 측정하는 방법
* 유클리디안 거리 (Euclidean Distance)
  * 유클리디안 거리 공식은 n차원의 공간에서 두 점간의 거리를 알아내는 공식이다
* 코사인 유사도 (Cosine Similarity)
  * 내적공간의 두 벡터간 각도의 코사인값을 이용하여 측정된 벡터간의 유사한 정도
* 고유명사, punctuations 등 처리
* 불용어 (Stopword)
  * 문장 내에서는 자주 등장하지만 문장을 분석하는 데 있어서는 큰 도움이 되지 않는 단어들



## 말뭉치 (Corpus) 텍스트 전처리

기본적인 순서: 수집 -> 시각화&전처리 -> 분석 -> 시각화 -> 알고리즘 선택 -> 모델링 -> 모델평가(피드백) -> 유지보수

```R
mytext<- c("software environment",
  "software  environment",
  "software\tenvironment")
mytext

# mytext에 있는 단어들을 공백을 기준으로 나누려고 한다.
library(stringr)
str_split(mytext, " ")    # 결과값 리스트 출력

  # sapply: 입력은 리스트, 출력은 벡터
  # laply: 입력은 리스트, 출력도 리스트

# 분리된 단어의 길이를 보려고 한다
sapply(str_split(mytext, " "), length)    # 출력 결과값 벡터
lapply(str_split(mytext, " "), length)    # 출력 결과값 리스트

# 각 리스트 요소에 저장된 문자열의 길이
sapply(str_split(mytext, " "), str_length)
  # length: 단어의 개수
  # str_length: 문자열의 글자수

# 공백 처리 과정

mytext    # 공백이 한개 이상 들어있다 (\t는 공백 4개와 같다). 공백을 다 한개로 통일해주면 위의 str_split으로 단어를 분리할 수 있다.
mytext.nowhitespace <- str_replace_all(mytext, "[[:space:]]{1,}", " ")    # [[:space:]]는 공백문자 의미
mytext.nowhitespace

sapply(str_split(mytext.nowhitespace, " "), length)
sapply(str_split(mytext.nowhitespace, " "), str_length)
```

### 대소문자 통일

```R
mytext <- "The 45th President of the United States, Donald Trump, states that he knows how to play trump with the former president"

# 단순히 단어별로 나눠보기

str_split(mytext, " ")    # 컴마 등 punctuation이 붙어있다
str_extract_all(mytext, boundary("word"))    # 컴마 등 punctuation이 없다

myword <- unlist(str_extract_all(mytext, boundary("word")))

# 고유명사 전처리

myword <- str_replace(myword, "Trump", "Trump_unique_")
myword <- str_replace(myword, "States", "States_unique_")

# 소문자 통일

tolower(myword)
table(tolower(myword))
```

### 숫자 처리

```R
mytext <- c("He is one of statisticians agreeing that R is the No. 1 statistical software.","He is one of statisticians agreeing that R is the No. one statistical software.")
str_split(mytext, " ")
mytext2 <- str_replace_all(mytext, "[[:digit:]]{1,}[[:space:]]{1,}","")    # 숫자를 없애려고 한다.
mytext2

mytext2 <- str_split(mytext2, " ")
mytext2

# mytext2의 각각의 단어를 공백으로 구분하여 합치려고 한다.
str_c(mytext2[[1]], collapse = " ")
str_c(mytext2[[2]], collapse = " ")

# 의미가 중요한 숫자의 경우 치환해준다.
# mytext에서 숫자는 모두 _number_로 일괄 치환
mytext3 <- str_replace_all(mytext, "[[:digit:]]{1,}[[:space:]]{1,}", "_number_ ")
mytext3

# mytext3 분리
mytext3 <- str_split(mytext3, " ")
mytext3
```

### punctuation 처리

```R
mytext <- "Baek et al. (2014) argued that the state of default-setting is critical for people to protect their own personal privacy on the Internet."
# "et al."에 있는 "."과 문장 끝에 있는 "."는 다르다.

str_split(mytext, " ")

# "."을 기준으로 분리
str_split(mytext, "\\. ")

# "-"를 " "으로 치환
mytext2 <- str_replace_all(mytext, "-", " ")
mytext2

# 성씨 다음 el al.이 오고, 이어서 (년도) 형식과 같은 패턴을 하나의 단어로 치환해보자. => "_reference_"로 일괄 치환하고자 함.

mytext2 <- str_replace_all(mytext2, 
                           "[[:upper:]]{1}[[:alpha:]]{1,}[[:space:]](et al\\.)[[:space:]]\\([[:digit:]]{4}\\)", 
                           "_reference_")
mytext2

# "."을 제거하는데, "." 뒤에 공백이 0개 이상인 경우 제거 (공백이 있건 없건 제거).

mytext2 <- str_replace_all(mytext2, 
                           "\\.[[:space:]]{0,}", 
                           "")
mytext2
```

### 불용어 처리

```R
# 불용어를 직접 등록해 불용어 제거

mystopwords <- "(\\ba )|(\\ban )|(\\bthe )"
mytext <- c("She is an actor", "She is the actor")
str_replace_all(mytext, mystopwords, "")

# 불용어 사전을 이용해 불용어 제거

library(tm)
stopwords("en")    # 짧은 불용어 목록
stopwords("SMART")    # 긴 불용어 목록
```

### 어근동일화 처리

* 동사는 시제를 고려해야 한다 => 동일화 해야 한다.
* ~s, ~es => 동일화
* 한국어의 경우에도 "가고", "가다", "간", "가니" 등등은 다 같은 말이므로 동일화 해아한다.

```R
# 영문에서 be동사에 해당하는 것들은 전부 다 be로 만들어버리기
mystemmer.func <- function(mytext){
  mytext <- str_replace_all(mytext, "(\\bam )|(\\bare )|(\\bis )|(\\bwas )|(\\bwere )|(\\bbe )", "be ")
  print(mytext)
}
test <- c("I am a boy. You are a boy. He might be a boy.")
mystemmer.func(test)
mytext.stem <- mystemmer.func(test)
# 결과 비교
table(str_split(test, " "))
table(str_split(mytext.stem, " "))
```

### n-gram

* n번 연이어 등장하는 단어들의 연결
* 2-gram을 bi-gram, 3-gram을 tri-gram이라고 부른다.
  * 예: "The 45th President of the United States, Donald Trump, states that he knows how to play trump with the former president"
    * bi-gram: "The 45th", "45th President", "President of", "of the", "the United", ...
    * tri-gram: "The 45th President", "45th President of", "President of the", "of the United", ...

* n-gram과 bayes theory(조건부 확률)을 통해 문맥을 파악한다.
  * 예: "45th President of" 뒤에 "the Unites States"가 올 확률을 구해 문맥을 파악

```R
mytext <- "The United States comprises fifty states. In the United States, each state has its own laws. However, federal law overrides state law in the United States."

# 단어 추출해보기

str_extract_all(mytext, boundary("word"))
# boundary 옵션들
str_extract_all(mytext, boundary("character"))
str_extract_all(mytext, boundary("sentence"))

myword <- unlist(str_extract_all(mytext, boundary("word")))
table(myword)
length(table(myword))
sum(table(myword))



# 고유명사는 미리 처리해준다

mytext.2gram <- str_replace_all(mytext, "\\bUnited States", "United_States")
str_extract_all(mytext.2gram, boundary("word"))
myword2 <- unlist(str_extract_all(mytext.2gram, boundary("word")))
myword2



# mytext의 단어들을 2단어씩 연결하여 출력해보세요. ("The United", "United States", "States comprises", ...)
mywords <- c()
for (i in 1:length(unlist(str_extract_all(mytext, boundary("word"))))-1) {
  mywords[i] <- paste(unlist(str_extract_all(mytext, boundary("word")))[i],
                      unlist(str_extract_all(mytext, boundary("word")))[i+1],
                      sep = " ")
}
mywords
```

### 말뭉치 구성

```r
library(tm)
my.text.location <- "Data/papers/"

mypaper <- VCorpus(DirSource(my.text.location))
mypaper

summary(mypaper)
class(mypaper)
mypaper[[1]]
mypaper[[2]]
mypaper[[2]]$content
mypaper[[3]]

mypaper[[2]]$meta
meta(mypaper[[2]], tag="author") <- "G. D. Hong"
mypaper[[2]]$meta
```

### "단어"+"특수문자"+"단어" 형식의 패턴 추출

```R
mypaper

myfunc <- function(x){
  str_extract_all(x, 
                  "[[:alnum:]]{1,}[[:punct:]]{1,}[[:alnum:]]{1,}")    # [[:alnum:]]은 알파벳 또는 숫자, [[:punct:]]는 특수문자
}

  # 함수는 호출을 해야만 실행된다. 반드시 어디선가 호출을 해줘야 한다.

mypunct <- lapply(mypaper,myfunc)
mypunct
unlist(mypunct)
table(unlist(mypunct))
```

#### 전체 문서에서 수치로 된  자료를 추출해서 mydigits에 넣기

```r
myfunc2 <- function(x){
  str_extract_all(x,
                  "[[:digit:]]{1,}")
}
mydigits <- lapply(mypaper, myfunc2)
mydigits
table(unlist(mydigits))
```

#### 대문자로 시작하는 단어 추출

```R
myfunc3 <- function(x){
  str_extract_all(x,
                  "\\b[[:upper:]]{1,}[[:alpha:]]{1,}")
}
myuppers <- lapply(mypaper,myfunc3)
myuppers
table(unlist(myuppers))
```

### 사실 이런 함수들은 이미 존재한다

```R
mycorpus <- tm_map(mypaper, removeNumbers)
mycorpus[[1]]$content

removePunctuation("hello......world")
```

### 어근 추출

```R
install.packages("SnowballC")
library(SnowballC)

wordStem(c("learn", "learns", "learning", "learned"))    # wordStem(): 어근 추출 함수

cleaned <- tm_map(mypaper, stemDocument)
cleaned[[1]]$content
```

### 문자열 전처리 코드들

```R
mytempfunc <- function(myobject, oldexp, newexp){
  newobject <- tm_map(myobject, 
                      content_transformer(function(x, pattern) gsub(pattern, newexp, x)), 
                      oldexp)
  print(newobject)
}

mycorpus <- mytempfunc(mycorpus,"-collar","collar")
mycorpus <- mytempfunc(mycorpus,"\\b((c|C)o-)","co")
mycorpus <- mytempfunc(mycorpus,"\\b((c|C)ross-)","cross")
mycorpus <- mytempfunc(mycorpus,"e\\.g\\.","for example")
mycorpus <- mytempfunc(mycorpus,"i\\.e\\.","that is")
mycorpus <- mytempfunc(mycorpus,"\\'s","")
mycorpus <- mytempfunc(mycorpus,"s’","s")
mycorpus <- mytempfunc(mycorpus,"ICD-","ICD")
mycorpus <- mytempfunc(mycorpus,"\\b((i|I)nter-)","inter")
mycorpus <- mytempfunc(mycorpus,"K-pop","Kpop")
mycorpus <- mytempfunc(mycorpus,"\\b((m|M)eta-)","meta")
mycorpus <- mytempfunc(mycorpus,"\\b((o|O)pt-)","opt")
mycorpus <- mytempfunc(mycorpus,"\\b((p|P)ost-)","post")
mycorpus <- mytempfunc(mycorpus,"-end","end")
mycorpus <- mytempfunc(mycorpus,"\\b((w|W)ithin-)","within")
mycorpus <- mytempfunc(mycorpus,"=","is equal to")
mycorpus <- mytempfunc(mycorpus,"and/or","and or")
mycorpus <- mytempfunc(mycorpus,"his/her","his her")
mycorpus <- mytempfunc(mycorpus,"-"," ")
```

### 양쪽 끝 공백 제거

```R
# stripWhitespace: 양쪽 끝 공백 제거
mycorpus <- tm_map(mycorpus, stripWhitespace)
mycorpus[[2]]$content
```

### 대소문자를 소문자로 일괄 치환

```R
mycorpus <- tm_map(mycorpus, content_transformer(tolower))    # tm함수 안에는 tolower같은 함수를 바로 쓸 수 없고 content_transformer로 담아줘야 한다.
# content_transformer는 객체가 함수로 처리되기에 적합하게끔 객체를 변환시켜준다.
mycorpus[[2]]$content
```

### 불용어 사전 적용해서 삭제

```R
mycorpus <- tm_map(mycorpus, removeWords, words = stopwords("SMART"))
mycorpus[[2]]$content
```

### 어근 동일화

```R
mycorpus <- tm_map(mycorpus, stemDocument, language = "en")
mycorpus[[2]]$content
```



## TF/IDF

TF/IDF

* TF/IDF는 결국 통계치이다.
* corpus에서 특정 단어가 얼마나 중요한지 알려주는 수치.
* 문서의 주제를 찾을 때 중요하다

### 문서*단어 행렬 (DTM) 만들기

```R
dtm.e <- DocumentTermMatrix(mycorpus)   # 가로: 문서, 세로: 단어
# TermDocumentMatrix() 는 가로가 단어, 세로가 문서
dtm.e
inspect(dtm.e[1:3,50:60])    # 행렬의 1~3행, 50~60열 보기
```




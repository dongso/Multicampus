# 복습

문서 변환

* 문서를 직접적으로 모델링하지는 못한다 (문자이기 때문에)
  * 수치화 해줘야 한다 => 벡터화
  * Vector Space Model
* 통일성 및 노이즈 제거를 위한 전처리 과정이 필요하다
  * 문장부호(.)를 제거
  * 문자를 모두 소문자로 변환
  * 불용어(stopwords) 제거
  * 어근 추출



# NGramTokenizer

NGramTokenizer: n-gram 토큰을 만들어준다

```R
install.packages("RWeka")
library(RWeka)

# NGramTokenizer: n-gram 토큰을 만들어준다

mytext <- c("The United States comprises fifty states.", "In the United States, each state has its own laws.", "However, federal law overrides state law in the United States.")
mytext

library(tm)
mytemp <- VCorpus(VectorSource(mytext))
# 이전에는 이렇게 했다: VCorpus(DirSource("경로"))
# 이번에는 VectorSource 사용
ngram.tdm <- TermDocumentMatrix(mytemp)
ngram.tdm
inspect(ngram.tdm)

bigramTokenizer <- function(x){
  NGramTokenizer(x, Weka_control(min=2, max=3))
}

ngram.tdm <- TermDocumentMatrix(mytemp, control =  list(tokenize=bigramTokenizer))

str(ngram.tdm)
ngram.tdm$dimnames$Terms

ngram.tdm
ngram.tdm[,]

bigramlist <- apply(ngram.tdm[,], 1, sum)    # apply 함수에는 array를 넣는다.
# 두번째 인수로는 margin이 들어가는데, 1은 행, 2는 열이라는 뜻이다.
# 세번째 인수는 적용시킬 함수이다.
# 결과적으로, 전체 문서에서 어떤 단어가 몇번 등장했는지가 결과로 나오게 된다.
sort(bigramlist, decreasing=T)
```



# 한국어 처리

```R
install.packages("KoNLP")
library(KoNLP)
library(stringr)
library(rJava)
```

## KoNLP 설치할 때 오류가 나는 경우 대처방안

1. KoNLP를 가장 먼저 로드한 다음, 나머지 패키지를 로드한다.

2. java 설정

   1. java 설치

   2. 내PC -> 고급설정 -> 시스템변수 -> 환경변수 추가 ->

      * 변수 이름: JAVA_HOME

      * 변수 값: JAVA 설치 경로 지정

   3. 확인 -> 종료

   4. RStudio 실행 -> 패키지 설치하고 로드

3. 그래도 에러가 발생한다면, RStudio에서 아래 코딩

   * Sys.setenv(JAVA_HOME="java경로")

```R
mytextlocation <- "Data/논문/"
# 파일들을 모두 가져와서 말뭉치 생성
mypaper <- VCorpus(DirSource(mytextlocation))
mypaper

# 내용 확인
mypaper[[1]]$content
mypaper[[19]]$content
```

## 한국어와 영어의 차이?

* 분석 단위
  * 영어의 경우에는 띄어쓰기로 단어를 분리할 수 있다
  * 한국어에는 조사가 있고 어미의 변화가 다양하다
* 데이터 처리 단위
  * 영어: 공란, 특수문자, 숫자, 불용어, 대소문자 통일, 어근 동일화, n-gram
  * 한국어: 공란, 특수문자, 숫자, 불용어(영어와 달리 불용어 사전 아직 없음), 명사 추출

```R
# 19번 논문 전처리 - 내용을 보고 뭘 처리해야할지 생각해보자
# 영단어 없애기, 소괄호 없애기, 중간점 없애기, 따옴표 없애기 등

mykorean <- mypaper[[19]]$content

#영문자 제거
mytext <- str_replace_all(mykorean, "[[:lower:]]", "")    # R에서는 [[:alpha:]]에 한글도 포함
# 또는
# mytext <- str_replace_all(mykorean, "[a-zA-Z]", "")
mytext <- str_replace_all(mytext, "\\(", "")
mytext <- str_replace_all(mytext, "\\)", "")
mytext

# 만약
# mytext <- str_replace_all(mykorean, "[가-힣]", "")
# 이면 모든 한글 제거

# 가운데 점이랑 홑따옴표 제거
mytext <- str_replace_all(mytext, " · ", "")    # 가운데점이랑 홑따옴표는 "\\"가 필요없다
mytext <- str_replace_all(mytext, "‘", "")
mytext <- str_replace_all(mytext, "’", "")
mytext

# 명사 추출
extractNoun(mytext)
noun.mytext <- extractNoun(mytext)
table(noun.mytext)

# 숫자 표현 추출
mypaper    # corpus 타입은 리스트로 보면 된다
mydigits <- lapply(mypaper, function(x) (str_extract_all(x, "[[:digit:]]{1,}")))
mydigits
unlist(mydigits)
table(unlist(mydigits))

# 숫자 제거
mycorpus <- tm_map(mypaper, removeNumbers)
str(mycorpus)
mycorpus[[1]]
inspect(mycorpus[[1]])


mypuncts <- lapply(mypaper, function(x)(str_extract_all(x,"\\b[[:alpha:]]{1,}[[:punct:]]{1,}[[:alpha:]]{1,}\\b")))
table(unlist(mypuncts))
```

## 함수 만들어 데이터 전처리 연습

```R
mytempfunct <- function(myobject, oldexp, newexp){
  tm_map(myobject, 
         content_transformer(function(x, pattern) gsub(pattern, newexp, x)), 
         oldexp)
}

mycorpus <- mytempfunct(mycorpus, "[[:lower:]]", "")
mycorpus <- mytempfunct(mycorpus, "[[:upper:]]", "")
mycorpus <- mytempfunct(mycorpus, "\\(", "")
mycorpus <- mytempfunct(mycorpus, "\\)", "")
mycorpus <- mytempfunct(mycorpus, "‘", "")
mycorpus <- mytempfunct(mycorpus, "’", "")
mycorpus <- mytempfunct(mycorpus, " · ", "")
mycorpus <- mytempfunct(mycorpus, "·", "")
mycorpus <- mytempfunct(mycorpus, "-", "")
mycorpus <- mytempfunct(mycorpus, "_", "")
mycorpus <- mytempfunct(mycorpus, "\\?", "")
mycorpus <- mytempfunct(mycorpus, "/", "")
mycorpus <- mytempfunct(mycorpus, "\\.", "")
mycorpus <- mytempfunct(mycorpus, ",", "")
mycorpus <- tm_map(mycorpus, stripWhitespace)

mycorpus[[3]]$content
inspect(mycorpus)


myNounFun <- function(mytext){
  print(paste(extractNoun(mytext), collapse = " "))
}
mycorpus[[1]]$content
myNounFun(mycorpus[[1]]$content)   # mycorpus[[1]] 내용에서 명사 리스트 출력



length(mycorpus)
myNounCorpus <- mycorpus
for (i in 1:length(mycorpus)){
  myNounCorpus[[i]]$content <- myNounFun(mycorpus[[i]]$content)
}

myNounCorpus[[19]]$content

lapply(myNounCorpus, function(x) str_extract_all(x, boundary("word")))
table(unlist(lapply(myNounCorpus, function(x) str_extract_all(x, boundary("word")))))    # 19개 문서 전체에 대한 단어 빈도


# "커뮤니"로 시작하는 단어는 전무 "커뮤니케이션"으로 바꾸고 싶다

imsi <- myNounCorpus
for (i in 1: length(myNounCorpus)){
  myNounCorpus[[i]]$content <- str_replace_all(imsi[[i]]$content, "커뮤니[[:alpha:]]{1,}", "커뮤니케이션")
  myNounCorpus[[i]]$content <- str_replace_all(imsi[[i]]$content, "위키리크스[[:alpha:]]{1,}", "위키리크스")
}

dtm.k <- DocumentTermMatrix(myNounCorpus)
dtm.k
colnames(dtm.k)    # 명사들 목록이 나온다
```



## 기술 통계

```R
word.freq <- apply(dtm.k[,], 2, sum)
head(word.freq)
length(word.freq)
sort(word.freq, decreasing = T)
sort.word.freq <- sort(word.freq, decreasing = T)
sort.word.freq[1:20]

cumsum.word.freq <- cumsum(sort.word.freq)
cumsum.word.freq[1:20]

cumsum.word.freq
length(cumsum.word.freq)
cumsum.word.freq[length(cumsum.word.freq)]    # 전체 누적합
cumsum.word.freq/cumsum.word.freq[length(cumsum.word.freq)]
# 상위 20개 단어들이 전체의 몇 퍼센트를 차지할까?
prob.word.freq <- cumsum.word.freq/cumsum.word.freq[length(cumsum.word.freq)]
prob.word.freq[1:20]    # 즉, 상위 20개 단어들이 전체의 약 39.9퍼센트를 차지한다.

# 시각화
plot(1:length(word.freq), prob.word.freq, type = "l")
library(wordcloud)
library(RColorBrewer)
mypal = brewer.pal(8, "Dark2")
wordcloud(names(word.freq), 
          freq = word.freq, 
          min.freq = 5, 
          col = mypal, 
          random.order = F, 
          scale = c(4,0.2))    # 19개 문서들에서 명사들만 추출했을 때 5번 이상 등장한 단어들에 대해 Word cloud를 만든 것
```



# 머신러닝

학습의 종류

* 교사 학습
* 비교사 학습
  * 문제만 있고 답은 없다
  * 비교사 학습의 대표적인 알고리즘이 kmeans
* 강화 학습

## kmeans

비교사학습의 한 방법

### 데이터 전처리

```R
raw_teens <- read.csv("Data/sns.csv")
teens <- raw_teens

str(teens)
table(teens$gender, useNA = "ifany")
summary(teens$age)

teens$age <- ifelse(teens$age>=13 & teens$age<20, 
                    teens$age, 
                    NA)
summary(teens$age)

# 결측치 처리 시 유의할 점

teens$female <- ifelse(teens$gender=="F", 1, 0)
table(teens$female)    # NA는 누락되고 F랑 M만 처리되었다

teens$female <- ifelse(teens$gender=="F" & !is.na(teens$gender), 1, 0)
table(teens$female)    # NA도 0에 포함되었다

# 성별이 없는 사람만 변수를 따로 만들려고 한다.

teens$nogender <- ifelse(is.na(teens$gender), 1, 0)
table(teens$gender, useNA = "ifany")
table(teens$female)
table(teens$nogender)

# 나이 평균 구하기

mean(teens$age)    # NA를 처리하지 않았기 때문에 계산되지 않는다
mean(teens$age, na.rm = T)

table(teens$gradyear)
aggregate(data=teens, age~gradyear, mean)    # 졸업년도에 따른 나이 평균



class(aggregate(data=teens, age~gradyear, mean))
# 위 데이터프레임과 같은 내용을 벡터로 출력
ave_age <- ave(teens$age, 
    teens$gradyear, 
    FUN = function(x) mean (x, na.rm = T))    # 졸업년도에 따라 age 평균 구하기
ave_age
class(ave_age)    # numeric vector
teens$age <- ifelse(is.na(teens$age), ave_age, teens$age)

summary(teens$age)



str(teens)
interests <- teens[5:40]
```

### 정규화와 표준화

1. 정규화
   * = (각 값 - 그 열의 최소값)/(그 열의 최대값 - 그 열의 최소값)
   * 0~1 사이로 설정
2. 표준화
   * = (각 값 - 평균)/표준편차 = Z점수
   * 표준(평균)을 기준으로 얼마나 떨어져있느냐를 나타낸다.
   * 비교대상이 서로 다른 단위일 때 사용 (예: 내 토익과 토플 점수를 비교하려고 할 때)

#### 표준화

```R
lapply(interests, scale)    # 표준화
interest_z <- as.data.frame(lapply(interests, scale))
interest_z
```

### 클러스터링

```R
set.seed(2345)
teen_clusters <- kmeans(interest_z, 5)
teen_clusters$size
teen_clusters$centers

teens$cluster <- teen_clusters$cluster
str(teens)

teen_clusters$clustercen <- teen_clusters$centers
str(teen_clusters)
teen_clusters$cluster    # 소속된 cluster의 번호가 다 나온다.
class(teen_clusters$cluster)
table(teen_clusters$cluster)

# 각 클러스터에 대한 나이의 평균을 출력해보기
aggregate(data=teens, age~cluster, mean)

# 각 클러스터에 대한 female의 평균 출력
aggregate(data=teens, female~cluster, mean)

# 각 클러스터에 대한 friends의 평균 출력
aggregate(data=teens, friends~cluster, mean)
```



